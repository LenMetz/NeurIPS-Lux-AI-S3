{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e05923d-4e29-40f2-8cd5-c760dbe03685",
   "metadata": {},
   "source": [
    "## PPO design notes\n",
    "\n",
    "### Actor/critic networks design\n",
    "### Env\n",
    "-convert obs to proper obs\n",
    "\n",
    "-reward calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10b8aec7-4238-4cac-97da-335e89aec5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from IPython.display import display, Javascript\n",
    "from luxai_s3.wrappers import LuxAIS3GymEnv, RecordEpisode\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "from my_agent.lux.utils import direction_to, direction_to_change\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from maps import EnergyMap, RelicMap, TileMap\n",
    "from astar import *\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import MultiDiscrete, Discrete, Tuple\n",
    "from agent import Agent\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a468f572-cd3c-4ae8-bb5b-5096d92db788",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    exp_name: str =\"\"\n",
    "    \"\"\"the name of this experiment\"\"\"\n",
    "    seed: int = 42\n",
    "    \"\"\"seed of the experiment\"\"\"\n",
    "    torch_deterministic: bool = True\n",
    "    \"\"\"if toggled, `torch.backends.cudnn.deterministic=False`\"\"\"\n",
    "    cuda: bool = False\n",
    "    \"\"\"if toggled, cuda will be enabled by default\"\"\"\n",
    "    capture_video: bool = False\n",
    "    \"\"\"whether to capture videos of the agent performances (check out `videos` folder)\"\"\"\n",
    "\n",
    "    # Algorithm specific arguments\n",
    "    env_id: str = \"CartPole-v1\"\n",
    "    \"\"\"the id of the environment\"\"\"\n",
    "    total_timesteps: int = 1000000\n",
    "    \"\"\"total timesteps of the experiments\"\"\"\n",
    "    learning_rate_actor: float = 1e-4\n",
    "    \"\"\"the learning rate of the actor optimizer\"\"\"\n",
    "    learning_rate_critic: float = 1e-3\n",
    "    \"\"\"the learning rate of the critic optimizer\"\"\"\n",
    "    num_envs: int=32\n",
    "    \"\"\"the number of parallel game environments\"\"\"\n",
    "    num_steps: int = 505\n",
    "    \"\"\"the number of steps to run in each environment per policy rollout\"\"\"\n",
    "    anneal_lr: bool = False\n",
    "    \"\"\"Toggle learning rate annealing for policy and value networks\"\"\"\n",
    "    anneal_clip: bool = True\n",
    "    \"\"\"Toggle learning rate annealing for policy and value networks\"\"\"\n",
    "    gamma: float = 0.99\n",
    "    \"\"\"the discount factor gamma\"\"\"\n",
    "    gae_lambda: float = 0.95\n",
    "    \"\"\"the lambda for the general advantage estimation\"\"\"\n",
    "    minibatch_size: int = 64\n",
    "    \"\"\"the number of mini-batches\"\"\"\n",
    "    update_epochs: int = 20\n",
    "    \"\"\"the K epochs to update the critic\"\"\"\n",
    "    update_epochs_critic: int = 40\n",
    "    \"\"\"the K epochs to update the critic\"\"\"\n",
    "    update_epochs_actor: int = 10\n",
    "    \"\"\"the K epochs to update the policy\"\"\"\n",
    "    norm_adv: bool = True\n",
    "    \"\"\"Toggles advantages normalization\"\"\"\n",
    "    clip_coef: float = 0.2\n",
    "    \"\"\"the surrogate clipping coefficient\"\"\"\n",
    "    clip_coef_start: float = 0.5\n",
    "    \"\"\"the surrogate clipping coefficient\"\"\"\n",
    "    clip_vloss: bool = True\n",
    "    \"\"\"Toggles whether or not to use a clipped loss for the value function, as per the paper.\"\"\"\n",
    "    ent_coef: float = 0.01\n",
    "    \"\"\"coefficient of the entropy\"\"\"\n",
    "    vf_coef: float = 0.5\n",
    "    \"\"\"coefficient of the value function\"\"\"\n",
    "    max_grad_norm: float = 0.5\n",
    "    \"\"\"the maximum norm for the gradient clipping\"\"\"\n",
    "    target_kl: float = 0.01\n",
    "    \"\"\"the target KL divergence threshold\"\"\"\n",
    "\n",
    "    # to be filled in runtime\n",
    "    batch_size: int = 0\n",
    "    \"\"\"the batch size (computed in runtime)\"\"\"\n",
    "    num_iterations: int = 0\n",
    "    \"\"\"the number of iterations (computed in runtime)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e16e823-e2c3-4958-babd-689b9ecf6f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.n_ens = env.observation_space[1].shape[0]\n",
    "        self.n_maps = len(env.single_observation_space[0])\n",
    "        self.n_state_params = env.single_observation_space[1].shape[0]\n",
    "        self.n_action = env.single_action_space.shape[0]\n",
    "        self.action_dim = env.single_action_space.nvec[-1,-1]\n",
    "        self.n_unit_states = env.single_observation_space[2].shape[1]\n",
    "        self.transformer_embedding_dim = env.get_attr(\"transformer_embedding_dim\")[0]\n",
    "        self.state_param_embedding_dim = env.get_attr(\"state_param_embedding_dim\")[0]\n",
    "        \n",
    "        self.cnn = nn.Sequential(\n",
    "            layer_init(nn.Conv2d(self.n_maps, 16, kernel_size=3, padding=1)),\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2),\n",
    "            layer_init(nn.Conv2d(16, 8, kernel_size=3, padding=1)),\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            layer_init(nn.Linear(576*8, 128)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(128, 1)),\n",
    "        )\n",
    "        \n",
    "        self.unit_net = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.n_unit_states, 32)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(32, 1)),\n",
    "        )\n",
    "    def get_value(self, x):\n",
    "        maps, state_params, unit_params = x\n",
    "        return torch.sum(self.unit_net(unit_params), dim=1) + self.cnn(maps)\n",
    "\n",
    "        \n",
    "# TODO network design\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.n_ens = env.observation_space[1].shape[0]\n",
    "        self.n_maps = len(env.single_observation_space[0])\n",
    "        self.n_state_params = env.single_observation_space[1].shape[0]\n",
    "        self.n_action = env.single_action_space.shape[0]\n",
    "        self.action_dim = env.single_action_space.nvec[-1,-1]\n",
    "        self.n_unit_states = env.single_observation_space[2].shape[1]\n",
    "        self.transformer_embedding_dim = env.get_attr(\"transformer_embedding_dim\")[0]\n",
    "        self.state_param_embedding_dim = env.get_attr(\"state_param_embedding_dim\")[0]\n",
    "        a = torch.tensor(np.stack((np.repeat(np.arange(24),24,axis=0).reshape((24,24)), np.repeat(np.arange(24),24,axis=0).reshape((24,24)).T),axis=2))\n",
    "        self.map_positions = torch.cat((a[:,:,0].view(576,1), a[:,:,1].view(576,1)),dim=1).unsqueeze(0)\n",
    "        self.state_params_to_hidden = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.n_state_params, 32)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(32, self.state_param_embedding_dim)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.embedding_maps = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.n_maps, 16)),\n",
    "            layer_init(nn.Linear(16, self.transformer_embedding_dim)),\n",
    "        )\n",
    "                \n",
    "        self.cnn = nn.Sequential(\n",
    "            layer_init(nn.Conv2d(self.n_maps, 32, kernel_size=3, padding=1)),\n",
    "            nn.ReLU(),\n",
    "            #layer_init(nn.Conv2d(16, 32, kernel_size=3, padding=1)),\n",
    "            #nn.ReLU(),\n",
    "            #nn.MaxPool2d(2),\n",
    "            layer_init(nn.Conv2d(32, self.transformer_embedding_dim-2, kernel_size=3, padding=1)),\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2),\n",
    "            nn.Flatten(start_dim=-2),\n",
    "            #layer_init(nn.Linear(6*6, 64)),\n",
    "            #nn.ReLU(),\n",
    "            #layer_init(nn.Linear(64, 32)),\n",
    "        )\n",
    "        \n",
    "        self.embedding_unit_params = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.n_unit_states, 32)),\n",
    "            layer_init(nn.Linear(32, self.transformer_embedding_dim)),\n",
    "        )\n",
    "        \n",
    "        self.actor_encoder = torch.nn.TransformerEncoder(torch.nn.TransformerEncoderLayer(self.transformer_embedding_dim,1,64, batch_first=True, dropout=0.0),num_layers=2)\n",
    "        self.actor_decoder = torch.nn.TransformerDecoder(torch.nn.TransformerDecoderLayer(self.transformer_embedding_dim,1,64, batch_first=True, dropout=0.0),num_layers=2)\n",
    "\n",
    "        self.out_to_logits = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.transformer_embedding_dim+self.state_param_embedding_dim, 256)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(256, 128)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(128, 2+4*24)),\n",
    "        )\n",
    "\n",
    "    def get_action(self, x, action=None, verbose=0):\n",
    "        maps, state_params, unit_params = x\n",
    "        #maps = torch.flatten(maps,start_dim=-2).permute(0,2,1)\n",
    "        batch_size, n_units = unit_params.shape[0], unit_params.shape[1] # B, N\n",
    "        map_positions = self.map_positions.repeat(batch_size,1,1)\n",
    "        #encoder_out = self.actor_encoder(self.embedding_maps(maps)) # B x 576 x 16\n",
    "        cnn_out = self.cnn(maps).swapaxes(-1,-2)\n",
    "        #kv = self.cnn_to_kv(cnn_out)\n",
    "        decoder_out = self.actor_decoder(self.embedding_unit_params(unit_params), torch.cat((map_positions,cnn_out),dim=-1)) # B x N x 16\n",
    "        \n",
    "        state_params_hidden = self.state_params_to_hidden(state_params) # B x 8\n",
    "        decoder_out_state_params_combined = torch.cat((decoder_out, torch.stack([state_params_hidden for i in range(n_units)],dim=1)),dim=-1) # B x N x 24\n",
    "        all_logits = self.out_to_logits(decoder_out_state_params_combined) # B x N x 2+4*24\n",
    "\n",
    "        move_type_logits = all_logits[:,:,:2].unsqueeze(-2) # B x N x 1 x 2\n",
    "        target_logits = all_logits[:,:,2:].view(batch_size, n_units, 4, self.action_dim) # B x N x 4 x 24\n",
    "        \n",
    "        move_type_probs = Categorical(logits=move_type_logits)\n",
    "        target_probs = Categorical(logits=target_logits)\n",
    "        \n",
    "        if action is None:\n",
    "            action_type = move_type_probs.sample()\n",
    "            action_target = target_probs.sample()\n",
    "            action = torch.cat((action_type,action_target),dim=-1) # B x N x 5\n",
    "        else:\n",
    "            action_type = action[:,:,0].unsqueeze(dim=-1)\n",
    "            action_target = action[:,:,1:]\n",
    "        probs = torch.cat((move_type_probs.log_prob(action_type), target_probs.log_prob(action_target)),dim=-1) # B x N x 5\n",
    "        return action, probs, move_type_probs.entropy() + target_probs.entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d098b1d-d3a8-4839-8ae1-23e58c93381f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_to_move(actor, envs, device=\"cpu\"):\n",
    "    optimizer = optim.Adam(actor.parameters(), lr=0.1, eps=1e-5)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    mask = torch.zeros((16,5))\n",
    "    mask[:,0] = 1\n",
    "    for i in range(10):\n",
    "        x = (torch.randint(0,100,((16,)+np.array(envs.single_observation_space[0]).shape)).to(torch.float).to(device),\n",
    "           torch.randint(0,100,((16,)+envs.single_observation_space[1].shape)).to(torch.float).to(device),\n",
    "          torch.randint(0,100,((16,)+np.array(envs.single_observation_space[2]).shape)).to(torch.float).to(device))\n",
    "        y = torch.rand((16,16,5))\n",
    "        y[:,:,0] = 0\n",
    "        y = torch.log(y)\n",
    "        action, pred,_ = actor.get_action(x)\n",
    "        y[action[:,:,0]==0] = 0\n",
    "        y[action[:,:,0]==1] = -1e10\n",
    "        loss = loss_fn(mask*y,mask*pred)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    return actor\n",
    "\n",
    "def train(exp_name, args, curriculum_step = 0, load_path=None, verbose=0):\n",
    "    args.exp_name = exp_name\n",
    "    args.batch_size = int(args.num_envs * args.num_steps)\n",
    "    args.num_iterations = args.total_timesteps // args.batch_size\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    run_name = f\"{args.exp_name}__{args.seed}__cstep_{curriculum_step}__{timestamp}\"\n",
    "    writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "    writer.add_text(\n",
    "        \"hyperparameters\",\n",
    "        \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
    "    )\n",
    "\n",
    "    # TRY NOT TO MODIFY: seeding\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "\n",
    "    # env setup\n",
    "    envs = gym.vector.SyncVectorEnv(\n",
    "        [env_fn for i in range(args.num_envs)],\n",
    "        copy=False\n",
    "    )\n",
    "    envs.set_attr(\"curriculum_step\", curriculum_step)\n",
    "    \n",
    "\n",
    "    actor = Actor(envs).to(device)\n",
    "    critic = Critic(envs).to(device)\n",
    "\n",
    "    if curriculum_step==-1:\n",
    "        actor = pretrain_to_move(actor, envs)\n",
    "    \n",
    "    if load_path is not None:\n",
    "        weights = torch.load(model_name, weights_only=True)\n",
    "        actor.load_state_dict(weights[\"actor\"])\n",
    "        critic.load_state_dict(weights[\"critic\"])\n",
    "\n",
    "    \n",
    "        \n",
    "    optimizer_actor = optim.Adam(actor.parameters(), lr=args.learning_rate_actor, eps=1e-8)\n",
    "    optimizer_critic = optim.Adam(critic.parameters(), lr=args.learning_rate_critic, eps=1e-8)\n",
    "        \n",
    "    # ALGO Logic: Storage setup\n",
    "\n",
    "    # TRY NOT TO MODIFY: start the game\n",
    "    global_step = 0\n",
    "    save_thresh = 100000\n",
    "    clip = args.clip_coef\n",
    "    start_time = time.time()\n",
    "    next_done = torch.zeros(args.num_envs).to(device)\n",
    "    for iteration in range(1, args.num_iterations + 1):\n",
    "        obs = (torch.zeros((args.num_steps, args.num_envs) + np.array(envs.single_observation_space[0]).shape).to(device),\n",
    "               torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space[1].shape).to(device),\n",
    "              torch.zeros((args.num_steps, args.num_envs) + np.array(envs.single_observation_space[2]).shape).to(device))\n",
    "        actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)\n",
    "        logprobs = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)\n",
    "        rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "        points = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "        dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "        values = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "        next_obs, _ = envs.reset(seed=args.seed)\n",
    "        next_obs = (torch.Tensor(np.array(next_obs[0])).to(device).reshape((args.num_envs,) + np.array(envs.single_observation_space[0]).shape),\n",
    "                    torch.Tensor(np.array(next_obs[1])).to(device).reshape((args.num_envs,)+envs.single_observation_space[1].shape),\n",
    "                    torch.Tensor(np.array(next_obs[2])).to(device).reshape((args.num_envs,) + np.array(envs.single_observation_space[2]).shape),\n",
    "                   )\n",
    "        # Annealing the rate if instructed to do so.\n",
    "        if args.anneal_lr:\n",
    "            frac = 1.0 - (iteration - 1.0) / args.num_iterations\n",
    "            lrnow_actor = frac * args.learning_rate_actor\n",
    "            optimizer_actor.param_groups[0][\"lr\"] = lrnow_actor\n",
    "            lrnow_critic = frac * args.learning_rate_critic\n",
    "            optimizer_critic.param_groups[0][\"lr\"] = lrnow_critic\n",
    "        if args.anneal_clip:\n",
    "            frac = (iteration - 1.0) / args.num_iterations\n",
    "            clip = args.clip_coef_start-(args.clip_coef_start-args.clip_coef)*frac\n",
    "        for step in range(0, args.num_steps):\n",
    "            with torch.no_grad():\n",
    "                global_step += args.num_envs\n",
    "                obs[0][step] = next_obs[0]\n",
    "                obs[1][step] = next_obs[1]\n",
    "                obs[2][step] = next_obs[2]\n",
    "                dones[step] = next_done\n",
    "                # ALGO LOGIC: action logic\n",
    "                action, logprob, _ = actor.get_action(next_obs)\n",
    "                value = critic.get_value(next_obs)\n",
    "                values[step] = value.flatten()\n",
    "                actions[step] = action\n",
    "                logprobs[step] = logprob\n",
    "    \n",
    "                # TRY NOT TO MODIFY: execute the game and log data.\n",
    "                next_obs, reward, terminations, truncations, infos = envs.step(action.cpu().numpy())\n",
    "                next_done = np.logical_or(terminations, truncations)\n",
    "                rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "                points[step] = torch.tensor(next_obs[1][:,6]).to(device).view(-1)\n",
    "                next_obs = (torch.Tensor(np.array(next_obs[0]), device=device).detach().swapaxes(0,1),\n",
    "                    torch.Tensor(np.array(next_obs[1]), device=device).detach(),\n",
    "                    torch.Tensor(np.array(next_obs[2]), device=device).detach(),\n",
    "                   )\n",
    "                next_done = torch.Tensor(next_done).to(device)\n",
    "        \n",
    "        envs.close()\n",
    "        writer.add_scalar(\"charts/reward\", torch.sum(rewards.detach()), global_step)\n",
    "        writer.add_scalar(\"charts/explore_rate\", 1-torch.mean(obs[0][:,:,0].detach()), global_step)\n",
    "        writer.add_scalar(\"charts/points\", torch.sum(points.detach()), global_step)\n",
    "        writer.add_scalar(\"charts/sap_%\", torch.mean(actions[:,:,:,0].detach()), global_step)\n",
    "        # bootstrap value if not done\n",
    "        with torch.no_grad():\n",
    "            next_value = critic.get_value(next_obs).reshape(1, -1)\n",
    "            advantages = torch.zeros_like(rewards).to(device)\n",
    "            lastgaelam = 0\n",
    "            for t in reversed(range(args.num_steps)):\n",
    "                if t == args.num_steps - 1:\n",
    "                    nextnonterminal = 1.0 - next_done\n",
    "                    nextvalues = next_value\n",
    "                else:\n",
    "                    nextnonterminal = 1.0 - dones[t + 1]\n",
    "                    nextvalues = values[t + 1]\n",
    "                delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]\n",
    "                advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n",
    "            returns = advantages + values\n",
    "        if verbose:\n",
    "            print(\"forward\", time.time()-tim)\n",
    "            tim = time.time()\n",
    "        # flatten the batch\n",
    "        b_obs = (obs[0].view((-1,) + np.array(envs.single_observation_space[0]).shape), \n",
    "                 obs[1].view((-1,) + envs.single_observation_space[1].shape),\n",
    "                 obs[2].view((-1,) + np.array(envs.single_observation_space[2]).shape), )\n",
    "        b_logprobs = logprobs.view((-1,) + envs.single_action_space.shape)\n",
    "        b_actions = actions.view((-1,) + envs.single_action_space.shape)\n",
    "        b_advantages = advantages.view(-1)\n",
    "        b_returns = returns.view(-1)\n",
    "        b_values = values.view(-1)\n",
    "        # Optimizing the policy and value network\n",
    "        b_inds = np.arange(args.batch_size)\n",
    "        clipfracs = []\n",
    "        approx_kl= 0\n",
    "        actor_updates = 0\n",
    "        policytimes = []\n",
    "        critictimes = []\n",
    "        for epoch in range(args.update_epochs):\n",
    "            np.random.shuffle(b_inds)\n",
    "            for start in range(0, args.batch_size, args.minibatch_size):\n",
    "                end = start + args.minibatch_size\n",
    "                mb_inds = b_inds[start:end]\n",
    "\n",
    "                tme = time.time()\n",
    "                # Policy update\n",
    "                if args.target_kl is None or approx_kl<args.target_kl:\n",
    "                    actor_updates = epoch*args.batch_size+end\n",
    "                    _, newlogprob, entropy = actor.get_action((b_obs[0][mb_inds],b_obs[1][mb_inds],b_obs[2][mb_inds]), b_actions[mb_inds])\n",
    "                    logratio = newlogprob - b_logprobs[mb_inds]\n",
    "                    ratio = logratio.exp()\n",
    "                    entropy_loss = entropy.mean()\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        old_approx_kl = (-logratio).mean()\n",
    "                        approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                        clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n",
    "    \n",
    "                    mb_advantages = b_advantages[mb_inds].view(-1,1,1)\n",
    "                    if args.norm_adv:\n",
    "                        mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "      \n",
    "                    pg_loss1 = -mb_advantages * ratio\n",
    "                    pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - clip, 1 + clip)\n",
    "                    pg_loss = torch.max(pg_loss1, pg_loss2).sum(dim=-1).sum(dim=-1).mean()                \n",
    "                    \n",
    "                    loss_actor = pg_loss - args.ent_coef * entropy_loss\n",
    "    \n",
    "                    optimizer_actor.zero_grad()\n",
    "                    loss_actor.backward()\n",
    "                    nn.utils.clip_grad_norm_(actor.parameters(), args.max_grad_norm)\n",
    "                    optimizer_actor.step()\n",
    "                #print(\"1 policy step\", time.time()-tme)\n",
    "                policytimes.append(time.time()-tme)\n",
    "                tme = time.time()\n",
    "                # Value update\n",
    "                newvalue = critic.get_value((b_obs[0][mb_inds],b_obs[1][mb_inds],b_obs[2][mb_inds]))\n",
    "                newvalue = newvalue.view(-1)\n",
    "                if args.clip_vloss:\n",
    "                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
    "                    v_clipped = b_values[mb_inds] + torch.clamp(\n",
    "                        newvalue - b_values[mb_inds],\n",
    "                        -args.clip_coef,\n",
    "                        args.clip_coef,\n",
    "                    )\n",
    "                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                    v_loss = 0.5 * v_loss_max.mean()\n",
    "                else:\n",
    "                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
    "                loss_critic = v_loss * args.vf_coef\n",
    "                #print(\"policy rest\", time.time()-tme)\n",
    "                #tme = time.time()\n",
    "                optimizer_critic.zero_grad()\n",
    "                loss_critic.backward()\n",
    "                nn.utils.clip_grad_norm_(critic.parameters(), args.max_grad_norm)\n",
    "                optimizer_critic.step()\n",
    "                critictimes.append(time.time()-tme)\n",
    "                #print(\"1 value step\", time.time()-tme)\n",
    "        if verbose:\n",
    "            print(np.sum(policytimes), np.sum(critictimes))\n",
    "            print(\"backwards\", time.time()-tim)\n",
    "            \n",
    "        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "        var_y = np.var(y_true)\n",
    "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "        # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
    "        writer.add_scalar(\"charts/learning_rate_actor\", optimizer_actor.param_groups[0][\"lr\"], global_step)\n",
    "        writer.add_scalar(\"charts/learning_rate_critic\", optimizer_critic.param_groups[0][\"lr\"], global_step)\n",
    "        writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n",
    "        writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n",
    "        writer.add_scalar(\"losses/epoch_to_kl\", actor_updates, global_step)\n",
    "        writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n",
    "        writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n",
    "        #print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "        writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "        if global_step>save_thresh:\n",
    "            torch.save({\"actor\" : actor.state_dict(),\n",
    "                        \"critic\" : critic.state_dict()}, \"models/\" + run_name + f\"_step_{global_step}\")\n",
    "            save_thresh += 100000\n",
    "        \n",
    "        obs = None\n",
    "        next_obs = None\n",
    "        actions = None\n",
    "        logprobs = None\n",
    "        rewards = None\n",
    "        points = None\n",
    "        dones = None\n",
    "        values = None\n",
    "    envs.close()\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc24dd79-2936-494c-ad9c-8738fcfc7386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_fn():\n",
    "    return ProxyEnvironment()\n",
    "\n",
    "class ProxyEnvironment(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.n_maps = 6\n",
    "        self.n_state_params = 3\n",
    "        self.transformer_embedding_dim = 16\n",
    "        self.state_param_embedding_dim = 8\n",
    "        self.map_space = Tuple((\n",
    "            MultiDiscrete(np.full((24,24),24)),\n",
    "            MultiDiscrete(np.full((24,24),24)),\n",
    "            MultiDiscrete(np.full((24,24),24)),\n",
    "            MultiDiscrete(np.full((24,24),24)),\n",
    "            MultiDiscrete(np.full((24,24),24)),\n",
    "            MultiDiscrete(np.full((24,24),24)),\n",
    "        ))\n",
    "        self.unit_param_space = MultiDiscrete(np.repeat(np.expand_dims(np.array([2,24,24,2,24,24,24,24,401,11,2]),0),16,axis=0),\n",
    "                                              start=np.repeat(np.expand_dims(np.array([0,0,0,0,0,0,0,0,0,-10,0]),0),16,axis=0))\n",
    "        self.param_space = MultiDiscrete(np.array([2,2,2,2,2,100,100,1000, 16*400,16]))\n",
    "        self.observation_space = Tuple((self.map_space, self.param_space, self.unit_param_space))\n",
    "        #print(self.observation_space)\n",
    "        self.action_space = MultiDiscrete(np.repeat(np.expand_dims(np.array([2,24,24,24,24]),0),16,axis=0))\n",
    "        self.current_step = 0\n",
    "        self.curriculum_step = 0\n",
    "        self.env = LuxAIS3GymEnv(numpy_output=True)\n",
    "        self.obs, info  = self.env.reset()\n",
    "        self.agent1 = ProxyAgent(\"player_0\", info[\"params\"], 0)\n",
    "        self.agent2 = Agent(\"player_1\", info[\"params\"])\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "        \n",
    "    def reset(self, seed=42, options=0):\n",
    "        self.current_step = 0\n",
    "        self.obs, info = self.env.reset(seed=seed)\n",
    "        self.agent1 = ProxyAgent(\"player_0\", info[\"params\"], self.curriculum_step)\n",
    "        self.agent2 = Agent(\"player_1\", info[\"params\"])\n",
    "        self.proxy_obs = self.agent1.get_init_proxy_obs(self.obs)\n",
    "        return self.proxy_obs, info\n",
    "\n",
    "    def step(self, proxy_action):\n",
    "        self.current_step += 1\n",
    "        actions = dict()\n",
    "        actions[\"player_0\"] = self.agent1.proxy_to_act(proxy_action)\n",
    "        actions[\"player_1\"] = self.agent2.act(step=self.current_step, obs=self.obs[self.agent2.player])\n",
    "        #print(self.obs[self.agent1.player])\n",
    "        self.obs, reward, terminated, truncated, info = self.env.step(actions)\n",
    "        terminated = terminated[\"player_0\"]\n",
    "        truncated = truncated[\"player_0\"]\n",
    "        #print(self.obs[self.agent1.player][\"units_mask\"])\n",
    "        self.proxy_obs, self.proxy_reward = self.agent1.step(self.obs[self.agent1.player], self.current_step)\n",
    "        return self.proxy_obs, self.proxy_reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddb399b4-fc30-422d-8252-0bca88f3248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProxyAgent():\n",
    "    def __init__(self, player: str, env_cfg, cr, model_name=None, inference=False) -> None:\n",
    "        self.cr = cr\n",
    "        self.player = player\n",
    "        self.opp_player = \"player_1\" if self.player == \"player_0\" else \"player_0\"\n",
    "        self.team_id = 0 if self.player == \"player_0\" else 1\n",
    "        self.opp_team_id = 1 if self.team_id == 0 else 0\n",
    "        np.random.seed(0)\n",
    "        self.env_cfg = env_cfg\n",
    "        if self.player==\"player_0\":\n",
    "            self.start_pos = [0,0]\n",
    "            self.pnum = 1\n",
    "        else:\n",
    "            self.start_pos = [23,23]\n",
    "            self.pnum = 0\n",
    "        self.unit_explore_locations = dict()\n",
    "        self.relic_node_positions = []\n",
    "        self.discovered_relic_nodes_ids = set()\n",
    "        self.n_units = self.env_cfg[\"max_units\"]\n",
    "        self.match_num = 1\n",
    "        self.relic_map = RelicMap(self.n_units)\n",
    "        self.tile_map = TileMap()\n",
    "        self.energy_map = EnergyMap()\n",
    "        self.move_cost = 3.0\n",
    "        self.nebula_drain = 5.0\n",
    "        self.move_check = 0\n",
    "        self.nebula_check = 0\n",
    "        \n",
    "        self.range = self.env_cfg[\"unit_sensor_range\"]\n",
    "        self.sap_range = self.env_cfg[\"unit_sap_range\"]\n",
    "        self.sap_cost = self.env_cfg[\"unit_sap_cost\"]\n",
    "        self.width = self.env_cfg[\"map_width\"]\n",
    "        self.height = self.env_cfg[\"map_height\"]\n",
    "        \n",
    "        self.unit_has_target = -np.ones((self.n_units))\n",
    "        self.unit_targets = np.zeros((self.n_units,2))\n",
    "        self.unit_targets_previous = dict(zip(range(0,self.n_units), np.zeros((self.n_units,2))))\n",
    "        self.unit_path = dict(zip(range(0,self.n_units), [[] for i in range(0,self.n_units)]))\n",
    "        self.unit_energys = np.full((self.n_units),100)\n",
    "        self.unit_positions = -np.ones((self.n_units,2))\n",
    "        self.available_unit_ids = []\n",
    "        self.unit_moved = np.zeros((self.n_units))\n",
    "        self.prev_points = 0\n",
    "        self.prev_point_diff = 0\n",
    "        self.prev_points_increase = 0\n",
    "        self.wins = 0\n",
    "        self.losses = 0\n",
    "        self.prev_actions = np.zeros((self.n_units,3))\n",
    "        self.prev_proxy_actions = np.zeros((self.n_units,5))\n",
    "        self.previous_energys = 100*np.ones((self.n_units))\n",
    "        self.previous_positions = -np.ones((self.n_units,2))\n",
    "        if inference:\n",
    "            self.actor = Actor(envs)\n",
    "            self.critic = Critic(envs)\n",
    "            envs.close()\n",
    "            if model_name:\n",
    "                checkpoint = torch.load(model_name, weights_only=True)\n",
    "                self.actor.load_state_dict(checkpoint[\"actor\"])\n",
    "                self.actor.eval()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.match_num += 1\n",
    "        self.unit_has_target = -np.ones((self.n_units))\n",
    "        self.unit_targets = np.zeros((self.n_units,2))\n",
    "        self.unit_targets_previous = dict(zip(range(0,self.n_units), np.zeros((self.n_units,2))))\n",
    "        self.unit_path = dict(zip(range(0,self.n_units), [[] for i in range(0,self.n_units)]))\n",
    "        self.available_unit_ids = []\n",
    "        self.unit_moved = np.zeros((self.n_units))\n",
    "        self.prev_points = 0\n",
    "        self.prev_point_diff = 0\n",
    "        self.prev_energy_total = self.n_units*100\n",
    "        self.prev_points_increase = 0\n",
    "        self.prev_actions = np.zeros((self.n_units,3))\n",
    "        self.prev_proxy_actions = np.zeros((self.n_units,5))\n",
    "        self.prev_energys = 100*np.ones((self.n_units))\n",
    "        self.previous_positions = -np.ones((self.n_units,2))\n",
    "\n",
    "    def compare_positions(self, pos1, pos2):\n",
    "        return pos1[0]==pos2[0] and pos1[1]==pos2[1]\n",
    "        \n",
    "    # bunnyhop mechanic (maximize points by avoiding doubling on fragment)\n",
    "    def bunnyhop(self, unit, unit_positions):\n",
    "        counter = 0\n",
    "        unit_pos = unit_positions[unit]\n",
    "        for unit2 in range(self.n_units):            \n",
    "            if self.unit_has_target[unit2]==2 and self.tile_map.map[unit_positions[unit2][0],unit_positions[unit2][1]]!=2 and len(self.unit_path[unit])>1 and self.compare_positions(self.unit_path[unit][0],unit_positions[unit2]):\n",
    "                self.unit_path[unit2] = self.unit_path[unit][1:]\n",
    "                self.unit_targets[unit2] = self.unit_targets[unit]\n",
    "                self.unit_has_target[unit2] = 1#self.unit_has_target[unit]\n",
    "                self.unit_path[unit] = [unit_positions[unit2]]\n",
    "                self.unit_targets[unit] = unit_positions[unit2]\n",
    "                self.unit_has_target[unit] = 1\n",
    "                counter +=1\n",
    "                if counter<10:\n",
    "                    self.bunnyhop(unit2, unit_positions)\n",
    "\n",
    "    def in_bounds(self, point):\n",
    "        return point[0]>0 and point[0]<24 and point[1]>0 and point[1]<24\n",
    "    \n",
    "    def positions_to_map(self, unit_positions):\n",
    "        if type(unit_positions)==dict:\n",
    "            unit_positions = np.array(list(unit_positions.items()))\n",
    "        unit_map = np.zeros((24,24))\n",
    "        for unit in unit_positions:\n",
    "            if unit[0]!=-1 and unit[1]!=-1:\n",
    "                unit_map[int(unit[0]),int(unit[1])] = 1\n",
    "        return unit_map\n",
    "\n",
    "    # adjust for not only direct hits, but adjacent hits\n",
    "    def check_hit(self, target):\n",
    "        for pos in self.enemy_positions:\n",
    "            if pos[0]!=-1 and pos[1]!=-1:\n",
    "                if pos[0]==target[0] and pos[1]==target[1]:\n",
    "                    return 1\n",
    "        else:\n",
    "            return 0\n",
    "    def get_explore_score(self, t):\n",
    "        score = 0\n",
    "        for x in range(-2,3):\n",
    "            for y in range(-2,3):\n",
    "                if self.in_bounds([t[0]+x,t[1]+y]):\n",
    "                    if self.tile_map.map[t[0]+x,t[1]+y]==-1:\n",
    "                        score +=1\n",
    "        #print(t)\n",
    "        return score\n",
    "\n",
    "    def get_close_known_score(self, pos_map):\n",
    "        weight = torch.tensor(np.array([[0.25,0.5,0.25],\n",
    "                  [0.5,1,0.5],\n",
    "                  [0.25,0.5,0.25]])).unsqueeze(0).unsqueeze(0)\n",
    "        b = torch.nn.functional.conv2d(torch.tensor(self.relic_map.map_knowns+self.relic_map.map_possibles).unsqueeze(0).unsqueeze(0), weight,padding=1)\n",
    "        c = torch.nn.functional.conv2d(b, weight,padding=1).squeeze()\n",
    "        return np.sum(pos_map * np.clip(c.numpy().round(2),a_min=None,a_max=3))\n",
    "    \n",
    "    def get_init_proxy_obs(self, obs):\n",
    "         return (np.array([np.zeros((24,24),dtype=int) for i in range(6)]),np.array([0 for i in range(10)]), np.zeros((self.n_units,11),dtype=int))\n",
    "     \n",
    "    def step(self, obs, step):        \n",
    "        if step in [101,202,303,404]:\n",
    "            self.reset()\n",
    "        reward = 0\n",
    "        unit_mask = np.array(obs[\"units_mask\"][self.team_id]) # shape (max_units, )\n",
    "        #print(step, unit_mask)\n",
    "        self.unit_positions = np.array(obs[\"units\"][\"position\"][self.team_id]) # shape (max_units, 2)\n",
    "        self.enemy_positions = np.array(obs[\"units\"][\"position\"][abs(self.team_id-1)]).tolist()\n",
    "        my_unit_map = self.positions_to_map(self.unit_positions)\n",
    "        enemy_unit_map = self.positions_to_map(self.enemy_positions)\n",
    "        self.unit_energys = np.array(obs[\"units\"][\"energy\"][self.team_id]) # shape (max_units, 1)\n",
    "        observed_relic_node_positions = np.array(obs[\"relic_nodes\"]) # shape (max_relic_nodes, 2)\n",
    "        observed_relic_nodes_mask = np.array(obs[\"relic_nodes_mask\"]) # shape (max_relic_nodes, )\n",
    "        team_points = np.array(obs[\"team_points\"]) # points of each team, team_points[self.team_id] is the points of the your team\n",
    "        increase = team_points[self.team_id]-self.prev_points\n",
    "        diff = team_points[self.team_id] - team_points[abs(self.team_id-1)]\n",
    "        diff_change = diff-self.prev_point_diff\n",
    "        self.prev_point_diff = diff\n",
    "        # ids of units you can control at this timestep\n",
    "        current_tile_map = obs[\"map_features\"][\"tile_type\"]\n",
    "        current_energy_map = obs[\"map_features\"][\"energy\"]\n",
    "        \n",
    "        # visible relic nodes\n",
    "        visible_relic_node_ids = set(np.where(observed_relic_nodes_mask)[0])\n",
    "        # save any new relic nodes that we discover for the rest of the game.\n",
    "        for ii in visible_relic_node_ids:\n",
    "            if ii not in self.discovered_relic_nodes_ids:\n",
    "                # explore units switch to relic collection\n",
    "                self.relic_map.new_relic(observed_relic_node_positions[ii])\n",
    "                self.discovered_relic_nodes_ids.add(ii)\n",
    "                self.discovered_relic_nodes_ids.add((ii+3)%6)\n",
    "                self.relic_node_positions.append(observed_relic_node_positions[ii])\n",
    "        \n",
    "\n",
    "\n",
    "        #print(self.unit_targets.items())\n",
    "        ### proxy reward calculation based on curriculum step###\n",
    "        # change in point difference \n",
    "        reward += increase\n",
    "        # win or lose\n",
    "        if obs[\"team_points\"][self.team_id]>self.wins:\n",
    "            self.wins = obs[\"team_points\"][self.team_id]\n",
    "            reward += 1000\n",
    "        if obs[\"team_points\"][abs(self.team_id-1)]>self.wins:\n",
    "            self.wins = obs[\"team_points\"][abs(self.team_id-1)]\n",
    "            reward += -1000\n",
    "        #reward += 0.1*np.sum(self.unit_energys-self.previous_energys)                \n",
    "        n_unknown_old = np.sum(1*(self.tile_map.map[1:23,1:23]!=-1))\n",
    "        #reward += n_unknown\n",
    "        # update maps\n",
    "        self.available_unit_ids = np.where(unit_mask)[0].tolist()\n",
    "        self.relic_map.step(self.unit_positions, increase)\n",
    "        tile_shift = self.tile_map.update(current_tile_map)\n",
    "        energy_shift = self.energy_map.update(current_energy_map)\n",
    "        n_unknown = np.sum(1*(self.tile_map.map[1:23,1:23]!=-1))\n",
    "        reward += 0.05*abs(n_unknown-n_unknown_old)\n",
    "        \n",
    "        \n",
    "        if self.cr>-1:\n",
    "            #reward += self.get_close_known_score(self.positions_to_map(self.unit_positions))\n",
    "            #reward += 0.5*self.get_close_known_score(self.positions_to_map(self.unit_targets))\n",
    "            #print(self.prev_proxy_actions.shape)\n",
    "            for unit in range(self.n_units):\n",
    "                pos = self.unit_positions[unit]\n",
    "                #if pos[0]!=-1 and pos[1]!=-1:\n",
    "                #print(\"\\n\\n\\n\",\"unit\", unit, \"prev ac\", self.prev_proxy_actions[unit][0])\n",
    "                #if self.prev_proxy_actions[unit][0]==1:\n",
    "                #    reward += 1\n",
    "                #else:\n",
    "                #if self.relic_map.map_knowns[pos[0],pos[1]]==1:\n",
    "                #    reward += 10\n",
    "                # units targeting possibles/known fragments\n",
    "                #t = self.unit_targets[unit]\n",
    "                #t = [int(t[0]),int(t[1])]\n",
    "                #e = 0.25*self.get_explore_score(t)\n",
    "                #reward += e\n",
    "                #if self.tile_map.map[int(t[0]),int(t[1])]==-1:\n",
    "                #    reward += 1\n",
    "                    #print(\"explore score\", e, \"target\", t)\n",
    "        #print(\"step\", step, \"reward\", reward, \"increase\", increase)\n",
    "            # unit dies (negative reward)\n",
    "            #else: \n",
    "            #    if self.unit_moved[unit]:\n",
    "            #        reward += -1\n",
    "            # hit enemy\n",
    "            #action = self.prev_actions[unit]\n",
    "            #print(action)\n",
    "            #if action[0]==5:\n",
    "            #    reward += self.check_hit(action[1:])\n",
    "            # collision\n",
    "            #if action[0]>0 and pos[0]==self.previous_positions[unit][0] and pos[0]==self.previous_positions[unit][1]:\n",
    "            #    reward += -10\n",
    "            #f self.compare_positions\n",
    "                \n",
    "            \n",
    "        \n",
    "\n",
    "        # find out move cost\n",
    "        if step>2 and not self.move_check and self.tile_map.map[self.unit_positions[0][0],self.unit_positions[0][1]]!=1 and self.unit_moved[0]:\n",
    "            self.move_cost=self.previous_energys[0]-self.unit_energys[0]+self.energy_map.map[self.unit_positions[0][0],self.unit_positions[0][1]]\n",
    "            self.move_check=1\n",
    "        # find out nebula drain\n",
    "        if not self.nebula_check and self.move_check:\n",
    "            for unit in self.available_unit_ids:\n",
    "                if self.unit_moved[unit] and  self.tile_map.map[self.unit_positions[unit][0],self.unit_positions[unit][1]]==1:\n",
    "                    self.nebula_check=1\n",
    "                    self.nebula_drain = -(self.unit_energys[unit]-self.previous_energys[unit]-self.energy_map.map[self.unit_positions[unit][0],self.unit_positions[unit][1]]+self.move_cost)\n",
    "                    break\n",
    "\n",
    "        \n",
    "        self.previous_energys = self.unit_energys\n",
    "        self.prev_points = team_points[self.team_id]\n",
    "        self.prev_points_increase = increase\n",
    "        self.previous_positions = self.unit_positions\n",
    "\n",
    "        # TODO explore map\n",
    "        tiles = np.zeros((24,24))\n",
    "        tiles[self.tile_map.map==-1] = 1\n",
    "        energy = self.energy_map.map.copy()\n",
    "        energy[self.tile_map.map==1] = energy[self.tile_map.map==1] - self.nebula_drain\n",
    "        on_known = np.zeros((self.n_units,1))\n",
    "        tile_energys = np.zeros((self.n_units,1))\n",
    "        for ii, p in enumerate(self.unit_positions):\n",
    "            if self.relic_map.map_knowns[p[0],p[1]]==1:\n",
    "                on_known[ii] = 1\n",
    "            tile_energys[ii] = energy[p[0],p[1]]\n",
    "        # constructing observations\n",
    "        # maps: unknown tile, energy, possibles, knowns, unit, enemy units\n",
    "        obs_maps = np.array([tiles.astype(int), energy.astype(int), self.relic_map.map_possibles.astype(int), self.relic_map.map_knowns.astype(int), my_unit_map.astype(int), enemy_unit_map.astype(int)])\n",
    "        # param: episode 1 hot, epi step, p diff, unit e, living units\n",
    "        episode = [0,0,0,0,0]\n",
    "        episode[int(max(0,(step-1)//101))] = 1\n",
    "        obs_params = np.array(episode+[(step-1)%101, increase, diff, np.sum(self.unit_energys), np.sum(1*(unit_mask))])\n",
    "        obs_units = np.concatenate((np.expand_dims(np.array(unit_mask),-1).astype(int), np.array(self.unit_positions).astype(int), self.prev_proxy_actions.astype(int), \n",
    "                                    np.expand_dims(self.unit_energys,-1).astype(int), tile_energys.astype(int), on_known.astype(int)), axis=-1)\n",
    "        proxy_obs = (obs_maps, \n",
    "                     obs_params, \n",
    "                     obs_units,\n",
    "                    )\n",
    "        return proxy_obs, reward\n",
    "        \n",
    "    def act(self, obs, step):\n",
    "        proxy_obs, _ = self.step(obs, step)\n",
    "        proxy_action,_ = self.model.get_value_and_action(proxy_obs)\n",
    "        return proxy_to_act(proxy_action)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def proxy_to_act(self, proxy_action):\n",
    "        if torch.is_tensor(proxy_action):\n",
    "            proxy_action = proxy_action.squeeze().cpu().detach().numpy()\n",
    "        actions = np.zeros((self.n_units, 3), dtype=int)\n",
    "        for unit in range(self.n_units):\n",
    "            if proxy_action[unit,0]==1:\n",
    "                actions[unit] = [5, proxy_action[unit,3], proxy_action[unit,4]]\n",
    "            else:\n",
    "                self.unit_targets[unit] = [proxy_action[unit,1],proxy_action[unit,2]]\n",
    "                '''if not self.compare_positions(self.unit_targets[unit], self.unit_targets_previous[unit]):\n",
    "                    path, _ = a_star(unit_positions[unit], self.unit_targets[unit], self.tile_map.map, self.energy_map.map, self.relic_map.map_knowns, self.move_cost, self.nebula_drain, use_energy=False)\n",
    "                    self.unit_path[unit] = path[1:]'''\n",
    "                direction = direction_to(self.unit_positions[unit], self.unit_targets[unit])\n",
    "                change = direction_to_change(direction)\n",
    "                self.unit_path[unit] = [self.unit_positions[unit][0]+change[0],self.unit_positions[unit][1]+change[1]]\n",
    "                actions[unit] = [direction, 0, 0]\n",
    "        self.prev_proxy_actions = proxy_action\n",
    "        self.prev_actions = actions\n",
    "        self.unit_targets_previous = self.unit_targets\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac52d86a-6597-4bc2-be01-5a52261b2f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenna\\anaconda3\\envs\\lux\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(\n",
      "c:\\Users\\lenna\\anaconda3\\envs\\lux\\Lib\\site-packages\\gymnasium\\spaces\\multi_discrete.py:214: UserWarning: \u001b[33mWARN: Getting the length of a multi-dimensional MultiDiscrete space.\u001b[0m\n",
      "  gym.logger.warn(\n",
      "c:\\Users\\lenna\\anaconda3\\envs\\lux\\Lib\\site-packages\\gymnasium\\spaces\\multi_discrete.py:214: UserWarning: \u001b[33mWARN: Getting the length of a multi-dimensional MultiDiscrete space.\u001b[0m\n",
      "  gym.logger.warn(\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "name = \"fixed_exp_test\"\n",
    "load_path = \"models/explore_harvest_model\"\n",
    "train(name, Args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c600b03-b0ea-496e-818b-8eab00e73ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cda7cc8-da37-4b47-a423-cd1843ae0a67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
