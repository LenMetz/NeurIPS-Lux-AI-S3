{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e05923d-4e29-40f2-8cd5-c760dbe03685",
   "metadata": {},
   "source": [
    "## PPO design notes\n",
    "\n",
    "### Actor/critic networks design\n",
    "### Env\n",
    "-convert obs to proper obs\n",
    "\n",
    "-reward calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10b8aec7-4238-4cac-97da-335e89aec5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from IPython.display import display, Javascript\n",
    "from luxai_s3.wrappers import LuxAIS3GymEnv, RecordEpisode\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "from my_agent.lux.utils import direction_to, direction_to_change\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from maps import EnergyMap, RelicMap, TileMap\n",
    "from astar import *\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import MultiDiscrete, Discrete, Tuple\n",
    "from agent import Agent\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a468f572-cd3c-4ae8-bb5b-5096d92db788",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    exp_name: str =\"\"\n",
    "    \"\"\"the name of this experiment\"\"\"\n",
    "    seed: int = 1\n",
    "    \"\"\"seed of the experiment\"\"\"\n",
    "    torch_deterministic: bool = True\n",
    "    \"\"\"if toggled, `torch.backends.cudnn.deterministic=False`\"\"\"\n",
    "    cuda: bool = False\n",
    "    \"\"\"if toggled, cuda will be enabled by default\"\"\"\n",
    "    capture_video: bool = False\n",
    "    \"\"\"whether to capture videos of the agent performances (check out `videos` folder)\"\"\"\n",
    "\n",
    "    # Algorithm specific arguments\n",
    "    env_id: str = \"CartPole-v1\"\n",
    "    \"\"\"the id of the environment\"\"\"\n",
    "    total_timesteps: int = 500000\n",
    "    \"\"\"total timesteps of the experiments\"\"\"\n",
    "    learning_rate_actor: float = 2.5e-4\n",
    "    \"\"\"the learning rate of the actor optimizer\"\"\"\n",
    "    learning_rate_critic: float = 2.5e-3\n",
    "    \"\"\"the learning rate of the critic optimizer\"\"\"\n",
    "    num_envs: int = 1\n",
    "    \"\"\"the number of parallel game environments\"\"\"\n",
    "    num_steps: int = 504\n",
    "    \"\"\"the number of steps to run in each environment per policy rollout\"\"\"\n",
    "    anneal_lr: bool = True\n",
    "    \"\"\"Toggle learning rate annealing for policy and value networks\"\"\"\n",
    "    gamma: float = 0.99\n",
    "    \"\"\"the discount factor gamma\"\"\"\n",
    "    gae_lambda: float = 0.95\n",
    "    \"\"\"the lambda for the general advantage estimation\"\"\"\n",
    "    minibatch_size: int = 32\n",
    "    \"\"\"the number of mini-batches\"\"\"\n",
    "    update_epochs: int = 80\n",
    "    \"\"\"the K epochs to update the policy\"\"\"\n",
    "    norm_adv: bool = True\n",
    "    \"\"\"Toggles advantages normalization\"\"\"\n",
    "    clip_coef: float = 0.3\n",
    "    \"\"\"the surrogate clipping coefficient\"\"\"\n",
    "    clip_vloss: bool = True\n",
    "    \"\"\"Toggles whether or not to use a clipped loss for the value function, as per the paper.\"\"\"\n",
    "    ent_coef: float = 0.01\n",
    "    \"\"\"coefficient of the entropy\"\"\"\n",
    "    vf_coef: float = 0.5\n",
    "    \"\"\"coefficient of the value function\"\"\"\n",
    "    max_grad_norm: float = 0.5\n",
    "    \"\"\"the maximum norm for the gradient clipping\"\"\"\n",
    "    target_kl: float = 0.01\n",
    "    \"\"\"the target KL divergence threshold\"\"\"\n",
    "\n",
    "    # to be filled in runtime\n",
    "    batch_size: int = 0\n",
    "    \"\"\"the batch size (computed in runtime)\"\"\"\n",
    "    num_iterations: int = 0\n",
    "    \"\"\"the number of iterations (computed in runtime)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e16e823-e2c3-4958-babd-689b9ecf6f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.n_ens = env.observation_space[1].shape[0]\n",
    "        self.n_maps = len(env.single_observation_space[0])\n",
    "        self.n_state_params = env.single_observation_space[1].shape[0]\n",
    "        self.n_action = env.single_action_space.shape[0]\n",
    "        self.action_dim = env.single_action_space.nvec[-1,-1]\n",
    "        self.n_unit_states = env.single_observation_space[2].shape[1]\n",
    "        self.transformer_embedding_dim = env.get_attr(\"transformer_embedding_dim\")[0]\n",
    "        self.state_param_embedding_dim = env.get_attr(\"state_param_embedding_dim\")[0]\n",
    "        \n",
    "        \n",
    "        self.critic_net = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.n_unit_states, 32)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(32, 1)),\n",
    "        )\n",
    "    def get_value(self, x):\n",
    "        maps, state_params, unit_params = x\n",
    "        return torch.sum(self.critic_net(unit_params).squeeze(), dim=-1)\n",
    "\n",
    "        \n",
    "# TODO network design\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.n_ens = env.observation_space[1].shape[0]\n",
    "        self.n_maps = len(env.single_observation_space[0])\n",
    "        self.n_state_params = env.single_observation_space[1].shape[0]\n",
    "        self.n_action = env.single_action_space.shape[0]\n",
    "        self.action_dim = env.single_action_space.nvec[-1,-1]\n",
    "        self.n_unit_states = env.single_observation_space[2].shape[1]\n",
    "        self.transformer_embedding_dim = env.get_attr(\"transformer_embedding_dim\")[0]\n",
    "        self.state_param_embedding_dim = env.get_attr(\"state_param_embedding_dim\")[0]\n",
    "        \n",
    "        self.state_params_to_hidden = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.n_state_params, 32)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(32, self.state_param_embedding_dim)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.embedding_maps = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.n_maps, 16)),\n",
    "            layer_init(nn.Linear(16, self.transformer_embedding_dim)),\n",
    "        )\n",
    "        \n",
    "        self.embedding_unit_params = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.n_unit_states, 16)),\n",
    "            layer_init(nn.Linear(16, self.transformer_embedding_dim)),\n",
    "        )\n",
    "        \n",
    "        self.actor_encoder = torch.nn.TransformerEncoderLayer(self.transformer_embedding_dim,4,64, batch_first=True)\n",
    "        self.actor_decoder = torch.nn.TransformerDecoderLayer(self.transformer_embedding_dim,4,64, batch_first=True)\n",
    "\n",
    "        self.out_to_logits = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.transformer_embedding_dim+self.state_param_embedding_dim, 32)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(32, 2+4*24)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def get_action(self, x, action=None):\n",
    "        maps, state_params, unit_params = x\n",
    "        maps = torch.flatten(maps,start_dim=-2).permute(0,2,1)\n",
    "        batch_size, n_units = unit_params.shape[0], unit_params.shape[1] # B, N\n",
    "        \n",
    "        encoder_out = self.actor_encoder(self.embedding_maps(maps)) # B x 576 x 16\n",
    "        decoder_out = self.actor_decoder(self.embedding_unit_params(unit_params), encoder_out) # B x N x 16\n",
    "        \n",
    "        state_params_hidden = self.state_params_to_hidden(state_params) # B x 8\n",
    "        decoder_out_state_params_combined = torch.cat((decoder_out, torch.stack([state_params_hidden for i in range(n_units)],dim=1)),dim=-1) # B x N x 24\n",
    "        all_logits = self.out_to_logits(decoder_out_state_params_combined) # B x N x 2+4*24\n",
    "        \n",
    "        move_type_logits = all_logits[:,:,:2].reshape(batch_size, n_units, 1, 2) # B x N x 1 x 2\n",
    "        target_logits = all_logits[:,:,2:].reshape(batch_size, n_units, 4, self.action_dim) # B x N x 4 x 24\n",
    "        move_type_probs = Categorical(logits=move_type_logits)\n",
    "        target_probs = Categorical(target_logits)\n",
    "\n",
    "        \n",
    "        if action is None:\n",
    "            action_type = move_type_probs.sample()\n",
    "            action_target = target_probs.sample()\n",
    "            action = torch.cat((action_type,action_target),dim=-1) # B x N x 5\n",
    "        else:\n",
    "            action_type = action[:,:,0].unsqueeze(dim=-1)\n",
    "            action_target = action[:,:,1:]\n",
    "        probs = torch.cat((move_type_probs.log_prob(action_type), target_probs.log_prob(action_target)),dim=-1) # B x N x 5\n",
    "        return action, probs, move_type_probs.entropy() + target_probs.entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fdbc6c3-589f-415a-8a22-74b3b9cc55c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.n_ens = env.observation_space[1].shape[0]\n",
    "        self.n_maps = len(env.single_observation_space[0])\n",
    "        self.n_state_params = env.single_observation_space[1].shape[0]\n",
    "        self.n_action = env.single_action_space.shape[0]\n",
    "        self.action_dim = env.single_action_space.nvec[-1,-1]\n",
    "        self.n_unit_states = env.single_observation_space[2].shape[1]\n",
    "        self.transformer_embedding_dim = env.get_attr(\"transformer_embedding_dim\")[0]\n",
    "        self.state_param_embedding_dim = env.get_attr(\"state_param_embedding_dim\")[0]\n",
    "        #self.transformer_embedding_dim = env[0].transformer_embedding_dim\n",
    "        \n",
    "        self.state_params_to_hidden = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.n_state_params, 32)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(32, self.state_param_embedding_dim)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.embedding_maps = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.n_maps, 16)),\n",
    "            layer_init(nn.Linear(16, self.transformer_embedding_dim)),\n",
    "        )\n",
    "        \n",
    "        self.embedding_unit_params = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.n_unit_states, 16)),\n",
    "            layer_init(nn.Linear(16, self.transformer_embedding_dim)),\n",
    "        )\n",
    "        \n",
    "        self.actor_encoder = torch.nn.TransformerEncoderLayer(self.transformer_embedding_dim,4,64, batch_first=True)\n",
    "        self.actor_decoder = torch.nn.TransformerDecoderLayer(self.transformer_embedding_dim,4,64, batch_first=True)\n",
    "        \n",
    "        self.critic_encoder = torch.nn.TransformerEncoderLayer(self.transformer_embedding_dim,4,64, batch_first=True)\n",
    "        self.critic_decoder = torch.nn.TransformerDecoderLayer(self.transformer_embedding_dim,4,64, batch_first=True)\n",
    "        self.out_to_logits = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.transformer_embedding_dim+self.state_param_embedding_dim, 32)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(32, 2+4*24)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.encoder_out_to_critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(8, 1)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.critic_out_old = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.transformer_embedding_dim, 32)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(32, 1)),\n",
    "        )\n",
    "        self.critic_out = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.n_unit_states, 32)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(32, 1)),\n",
    "        )\n",
    "        \n",
    "\n",
    "    def critic_old(self, maps, unit_params):\n",
    "        encoder_out = self.critic_encoder(self.embedding_maps(maps)) # B x 576 x 8\n",
    "        decoder_out = self.critic_decoder(self.embedding_unit_params(unit_params), encoder_out) # B x N x 8\n",
    "        return torch.sum(self.critic_out_old(decoder_out).squeeze(), dim=-1)\n",
    "\n",
    "    def critic(self, maps, unit_params):\n",
    "        return torch.sum(self.critic_out(unit_params).squeeze(), dim=-1)\n",
    "        \n",
    "        \n",
    "    def get_value(self, x):\n",
    "        maps, state_params, unit_params = x\n",
    "        maps = torch.flatten(maps,start_dim=-2).permute(0,2,1)\n",
    "        value = self.critic(maps, unit_params)\n",
    "        return value\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        maps, state_params, unit_params = x\n",
    "        maps = torch.flatten(maps,start_dim=-2).permute(0,2,1)\n",
    "        batch_size, n_units = unit_params.shape[0], unit_params.shape[1] # B, N\n",
    "        \n",
    "        encoder_out = self.actor_encoder(self.embedding_maps(maps)) # B x 576 x 16\n",
    "        decoder_out = self.actor_decoder(self.embedding_unit_params(unit_params), encoder_out) # B x N x 16\n",
    "        \n",
    "        state_params_hidden = self.state_params_to_hidden(state_params) # B x 8\n",
    "        decoder_out_state_params_combined = torch.cat((decoder_out, torch.stack([state_params_hidden for i in range(n_units)],dim=1)),dim=-1) # B x N x 24\n",
    "        all_logits = self.out_to_logits(decoder_out_state_params_combined) # B x N x 2+4*24\n",
    "        \n",
    "        move_type_logits = all_logits[:,:,:2].reshape(batch_size, n_units, 1, 2) # B x N x 1 x 2\n",
    "        target_logits = all_logits[:,:,2:].reshape(batch_size, n_units, 4, self.action_dim) # B x N x 4 x 24\n",
    "        move_type_probs = Categorical(logits=move_type_logits)\n",
    "        target_probs = Categorical(target_logits)\n",
    "\n",
    "        value = self.critic(maps, unit_params)\n",
    "        \n",
    "        if action is None:\n",
    "            action_type = move_type_probs.sample()\n",
    "            action_target = target_probs.sample()\n",
    "            action = torch.cat((action_type,action_target),dim=-1) # B x N x 5\n",
    "        else:\n",
    "            action_type = action[:,:,0].unsqueeze(dim=-1)\n",
    "            action_target = action[:,:,1:]\n",
    "        probs = torch.cat((move_type_probs.log_prob(action_type), target_probs.log_prob(action_target)),dim=-1) # B x N x 5\n",
    "        return action, probs, move_type_probs.entropy() + target_probs.entropy(), value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d098b1d-d3a8-4839-8ae1-23e58c93381f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(exp_name, args):\n",
    "    args.exp_name = exp_name\n",
    "    args.batch_size = int(args.num_envs * args.num_steps)\n",
    "    args.num_iterations = args.total_timesteps // args.batch_size\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    run_name = f\"{args.exp_name}__{args.seed}__{timestamp}\"\n",
    "    writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "    writer.add_text(\n",
    "        \"hyperparameters\",\n",
    "        \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
    "    )\n",
    "\n",
    "    # TRY NOT TO MODIFY: seeding\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "\n",
    "    # env setup\n",
    "    envs = gym.vector.SyncVectorEnv(\n",
    "        [env_fn for i in range(args.num_envs)],\n",
    "    )\n",
    "    \n",
    "\n",
    "    actor = Actor(envs).to(device)\n",
    "    critic = Critic(envs).to(device)\n",
    "    optimizer_actor = optim.Adam(actor.parameters(), lr=args.learning_rate_actor, eps=1e-5)\n",
    "    optimizer_critic = optim.Adam(critic.parameters(), lr=args.learning_rate_critic, eps=1e-5)\n",
    "    # ALGO Logic: Storage setup\n",
    "    obs = (torch.zeros((args.num_steps, args.num_envs) + np.array(envs.single_observation_space[0]).shape).to(device),\n",
    "           torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space[1].shape).to(device),\n",
    "          torch.zeros((args.num_steps, args.num_envs) + np.array(envs.single_observation_space[2]).shape).to(device))\n",
    "    actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)\n",
    "    logprobs = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)\n",
    "    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    values = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "\n",
    "    # TRY NOT TO MODIFY: start the game\n",
    "    global_step = 0\n",
    "    save_thresh = 100000\n",
    "    start_time = time.time()\n",
    "    next_obs, _ = envs.reset(seed=args.seed)\n",
    "    next_obs = (torch.Tensor(np.array(next_obs[0])).to(device).reshape((args.num_envs,) + np.array(envs.single_observation_space[0]).shape),\n",
    "                torch.Tensor(np.array(next_obs[1])).to(device).reshape((args.num_envs,)+envs.single_observation_space[1].shape),\n",
    "                torch.Tensor(np.array(next_obs[2])).to(device).reshape((args.num_envs,) + np.array(envs.single_observation_space[2]).shape),\n",
    "               )\n",
    "    next_done = torch.zeros(args.num_envs).to(device)\n",
    "    t = 0\n",
    "    for iteration in range(1, args.num_iterations + 1):\n",
    "        # Annealing the rate if instructed to do so.\n",
    "        if args.anneal_lr:\n",
    "            frac = 1.0 - (iteration - 1.0) / args.num_iterations\n",
    "            lrnow_actor = frac * args.learning_rate_actor\n",
    "            optimizer_actor.param_groups[0][\"lr\"] = lrnow_actor\n",
    "            lrnow_critic = frac * args.learning_rate_critic\n",
    "            optimizer_critic.param_groups[0][\"lr\"] = lrnow_critic\n",
    "            \n",
    "        for step in range(0, args.num_steps):\n",
    "            global_step += args.num_envs\n",
    "            obs[0][step] = next_obs[0]\n",
    "            obs[1][step] = next_obs[1]\n",
    "            obs[2][step] = next_obs[2]\n",
    "            dones[step] = next_done\n",
    "            # ALGO LOGIC: action logic\n",
    "            with torch.no_grad():\n",
    "                action, logprob, _ = actor.get_action(next_obs)\n",
    "                value = critic.get_value(next_obs)\n",
    "                values[step] = value.flatten()\n",
    "            actions[step] = action\n",
    "            logprobs[step] = logprob\n",
    "\n",
    "            # TRY NOT TO MODIFY: execute the game and log data.\n",
    "            next_obs, reward, terminations, truncations, infos = envs.step(action.cpu().numpy())\n",
    "            next_done = np.logical_or(terminations, truncations)\n",
    "            rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "            next_obs = (torch.Tensor(np.array(next_obs[0])).to(device).swapaxes(0,1),\n",
    "                torch.Tensor(np.array(next_obs[1])).to(device),\n",
    "                torch.Tensor(np.array(next_obs[2])).to(device),\n",
    "               )\n",
    "            next_done = torch.Tensor(next_done).to(device)\n",
    "            if \"final_info\" in infos:\n",
    "                for info in infos[\"final_info\"]:\n",
    "                    if info and \"episode\" in info:\n",
    "                        print(f\"global_step={global_step}, episodic_return={info['episode']['r']}\")\n",
    "                        writer.add_scalar(\"charts/episodic_return\", info[\"episode\"][\"r\"], global_step)\n",
    "                        writer.add_scalar(\"charts/episodic_length\", info[\"episode\"][\"l\"], global_step)\n",
    "        writer.add_scalar(\"charts/reward\", torch.sum(rewards), global_step)\n",
    "        # bootstrap value if not done\n",
    "        with torch.no_grad():\n",
    "            next_value = critic.get_value(next_obs).reshape(1, -1)\n",
    "            advantages = torch.zeros_like(rewards).to(device)\n",
    "            lastgaelam = 0\n",
    "            for t in reversed(range(args.num_steps)):\n",
    "                if t == args.num_steps - 1:\n",
    "                    nextnonterminal = 1.0 - next_done\n",
    "                    nextvalues = next_value\n",
    "                else:\n",
    "                    nextnonterminal = 1.0 - dones[t + 1]\n",
    "                    nextvalues = values[t + 1]\n",
    "                delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]\n",
    "                advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n",
    "            returns = advantages + values\n",
    "        # flatten the batch\n",
    "        b_obs = (obs[0].reshape((-1,) + np.array(envs.single_observation_space[0]).shape), \n",
    "                 obs[1].reshape((-1,) + envs.single_observation_space[1].shape),\n",
    "                 obs[2].reshape((-1,) + np.array(envs.single_observation_space[2]).shape), )\n",
    "        b_logprobs = logprobs.reshape((-1,) + envs.single_action_space.shape)\n",
    "        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "        b_advantages = advantages.reshape(-1)\n",
    "        b_returns = returns.reshape(-1)\n",
    "        b_values = values.reshape(-1)\n",
    "\n",
    "        # Optimizing the policy and value network\n",
    "        b_inds = np.arange(args.batch_size)\n",
    "        clipfracs = []\n",
    "        approx_kl = 0\n",
    "        actor_updates = 0\n",
    "        for epoch in range(args.update_epochs):\n",
    "            np.random.shuffle(b_inds)\n",
    "            for start in range(0, args.batch_size, args.minibatch_size):\n",
    "                end = start + args.minibatch_size\n",
    "                mb_inds = b_inds[start:end]\n",
    "\n",
    "                # Policy update\n",
    "                if approx_kl is not None and approx_kl<args.target_kl:\n",
    "                    actor_updates = epoch+1\n",
    "                    _, newlogprob, entropy = actor.get_action((b_obs[0][mb_inds],b_obs[1][mb_inds],b_obs[2][mb_inds]), b_actions.long()[mb_inds])\n",
    "                    logratio = newlogprob - b_logprobs[mb_inds]\n",
    "                    ratio = logratio.exp()\n",
    "                    entropy_loss = entropy.mean()\n",
    "            \n",
    "                    with torch.no_grad():\n",
    "                        old_approx_kl = (-logratio).mean()\n",
    "                        approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                        clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n",
    "    \n",
    "                    mb_advantages = b_advantages[mb_inds].view(-1,1,1)\n",
    "                    if args.norm_adv:\n",
    "                        mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "      \n",
    "                    pg_loss1 = -mb_advantages * ratio\n",
    "                    pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n",
    "                    pg_loss = torch.max(pg_loss1, pg_loss2).sum(dim=-1).sum(dim=-1).mean()                \n",
    "                    \n",
    "                    loss_actor = pg_loss - args.ent_coef * entropy_loss\n",
    "    \n",
    "                    optimizer_actor.zero_grad()\n",
    "                    loss_actor.backward()\n",
    "                    nn.utils.clip_grad_norm_(actor.parameters(), args.max_grad_norm)\n",
    "                    optimizer_actor.step()\n",
    "                \n",
    "                # Value update\n",
    "                newvalue = critic.get_value((b_obs[0][mb_inds],b_obs[1][mb_inds],b_obs[2][mb_inds]))\n",
    "                newvalue = newvalue.view(-1)\n",
    "                if args.clip_vloss:\n",
    "                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
    "                    v_clipped = b_values[mb_inds] + torch.clamp(\n",
    "                        newvalue - b_values[mb_inds],\n",
    "                        -args.clip_coef,\n",
    "                        args.clip_coef,\n",
    "                    )\n",
    "                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                    v_loss = 0.5 * v_loss_max.mean()\n",
    "                else:\n",
    "                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
    "                loss_critic = v_loss * args.vf_coef\n",
    "                optimizer_critic.zero_grad()\n",
    "                loss_critic.backward()\n",
    "                nn.utils.clip_grad_norm_(critic.parameters(), args.max_grad_norm)\n",
    "                optimizer_critic.step()\n",
    "\n",
    "            \n",
    "        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "        var_y = np.var(y_true)\n",
    "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "        # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
    "        writer.add_scalar(\"charts/learning_rate_actor\", optimizer_actor.param_groups[0][\"lr\"], global_step)\n",
    "        writer.add_scalar(\"charts/learning_rate_critic\", optimizer_critic.param_groups[0][\"lr\"], global_step)\n",
    "        writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n",
    "        writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n",
    "        writer.add_scalar(\"losses/epoch_to_kl\", actor_updates, global_step)\n",
    "        writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n",
    "        writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n",
    "        #print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "        writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "        if global_step>save_thresh:\n",
    "            torch.save({\"actor\" : actor.state_dict(),\n",
    "                        \"critic\" : critic.state_dict()}, \"models/\" + run_name + f\"_step_{global_step}\")\n",
    "            save_thresh += 100000\n",
    "\n",
    "    envs.close()\n",
    "    writer.close()\n",
    "    torch.save(agent.state_dict(), \"models/\" + run_name + \"_step_\" + iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc24dd79-2936-494c-ad9c-8738fcfc7386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_fn():\n",
    "    return ProxyEnvironment()\n",
    "\n",
    "class ProxyEnvironment(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.n_maps = 6\n",
    "        self.n_state_params = 3\n",
    "        self.transformer_embedding_dim = 8\n",
    "        self.state_param_embedding_dim = 8\n",
    "        self.map_space = Tuple((\n",
    "            MultiDiscrete(np.full((24,24),24)),\n",
    "            MultiDiscrete(np.full((24,24),24)),\n",
    "            MultiDiscrete(np.full((24,24),24)),\n",
    "            MultiDiscrete(np.full((24,24),24)),\n",
    "            MultiDiscrete(np.full((24,24),24)),\n",
    "            MultiDiscrete(np.full((24,24),24)),\n",
    "        ))\n",
    "        self.unit_param_space = MultiDiscrete(np.repeat(np.expand_dims(np.array([24,24,24,24,401,11,2]),0),16,axis=0),\n",
    "                                              start=np.repeat(np.expand_dims(np.array([0,0,0,0,0,-10,0]),0),16,axis=0))\n",
    "        self.param_space = MultiDiscrete(np.array([505, 1000, 16*400]))\n",
    "        self.observation_space = Tuple((self.map_space, self.param_space, self.unit_param_space))\n",
    "        #print(self.observation_space)\n",
    "        self.action_space = MultiDiscrete(np.repeat(np.expand_dims(np.array([2,24,24,24,24]),0),16,axis=0))\n",
    "        self.env = RecordEpisode(LuxAIS3GymEnv(numpy_output=True), save_on_close=False, save_on_reset=False, save_dir=\"replays\")\n",
    "        self.obs, info  = self.env.reset()\n",
    "        self.agent1 = ProxyAgent(\"player_0\", info[\"params\"])\n",
    "        self.agent2 = Agent(\"player_1\", info[\"params\"])\n",
    "        self.current_step = 0\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "        \n",
    "    def reset(self, seed=42, options=0):\n",
    "        self.current_step = 0\n",
    "        self.obs, info = self.env.reset(seed=seed)\n",
    "        self.agent1 = ProxyAgent(\"player_0\", info[\"params\"])\n",
    "        self.agent2 = Agent(\"player_1\", info[\"params\"])\n",
    "        self.proxy_obs = self.agent1.get_init_proxy_obs(self.obs)\n",
    "        return self.proxy_obs, info\n",
    "\n",
    "    def step(self, proxy_action):\n",
    "        self.current_step += 1\n",
    "        actions = dict()\n",
    "        actions[\"player_0\"] = self.agent1.proxy_to_act(proxy_action)\n",
    "        actions[\"player_1\"] = self.agent2.act(step=self.current_step, obs=self.obs[self.agent2.player])\n",
    "        #print(self.obs[self.agent1.player])\n",
    "        self.obs, reward, terminated, truncated, info = self.env.step(actions)\n",
    "        terminated = terminated[\"player_0\"]\n",
    "        truncated = truncated[\"player_0\"]\n",
    "        #print(self.obs[self.agent1.player][\"units_mask\"])\n",
    "        self.proxy_obs, self.proxy_reward = self.agent1.step(self.obs[self.agent1.player], self.current_step)\n",
    "        return self.proxy_obs, self.proxy_reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddb399b4-fc30-422d-8252-0bca88f3248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProxyAgent():\n",
    "    def __init__(self, player: str, env_cfg, model_name=None) -> None:\n",
    "        self.player = player\n",
    "        self.opp_player = \"player_1\" if self.player == \"player_0\" else \"player_0\"\n",
    "        self.team_id = 0 if self.player == \"player_0\" else 1\n",
    "        self.opp_team_id = 1 if self.team_id == 0 else 0\n",
    "        np.random.seed(0)\n",
    "        self.env_cfg = env_cfg\n",
    "        if self.player==\"player_0\":\n",
    "            self.start_pos = [0,0]\n",
    "            self.pnum = 1\n",
    "        else:\n",
    "            self.start_pos = [23,23]\n",
    "            self.pnum = 0\n",
    "        self.unit_explore_locations = dict()\n",
    "        self.relic_node_positions = []\n",
    "        self.discovered_relic_nodes_ids = set()\n",
    "        self.n_units = self.env_cfg[\"max_units\"]\n",
    "        self.match_num = 1\n",
    "        self.relic_map = RelicMap(self.n_units)\n",
    "        self.tile_map = TileMap()\n",
    "        self.energy_map = EnergyMap()\n",
    "        self.move_cost = 3.0\n",
    "        self.nebula_drain = 5.0\n",
    "        self.move_check = 0\n",
    "        self.nebula_check = 0\n",
    "        \n",
    "        self.range = self.env_cfg[\"unit_sensor_range\"]\n",
    "        self.sap_range = self.env_cfg[\"unit_sap_range\"]\n",
    "        self.sap_cost = self.env_cfg[\"unit_sap_cost\"]\n",
    "        self.width = self.env_cfg[\"map_width\"]\n",
    "        self.height = self.env_cfg[\"map_height\"]\n",
    "        \n",
    "        self.unit_has_target = -np.ones((self.n_units))\n",
    "        self.unit_targets = dict(zip(range(0,self.n_units), np.zeros((self.n_units,2))))\n",
    "        self.unit_targets_previous = dict(zip(range(0,self.n_units), np.zeros((self.n_units,2))))\n",
    "        self.unit_path = dict(zip(range(0,self.n_units), [[] for i in range(0,self.n_units)]))\n",
    "        self.unit_energys = np.full((self.n_units),100)\n",
    "        self.unit_positions = -np.ones((self.n_units,2))\n",
    "        self.available_unit_ids = []\n",
    "        self.unit_moved = np.zeros((self.n_units))\n",
    "        self.prev_points = 0\n",
    "        self.prev_point_diff = 0\n",
    "        self.prev_points_increase = 0\n",
    "        self.wins = 0\n",
    "        self.losses = 0\n",
    "        self.prev_actions = None\n",
    "        self.previous_energys = 100*np.zeros((self.n_units))\n",
    "        self.previous_positions = -np.ones((self.n_units,2))\n",
    "        self.model = None\n",
    "        if model_name:\n",
    "            self.model = torch.load(model_name)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.match_num += 1\n",
    "        self.unit_has_target = -np.ones((self.n_units))\n",
    "        self.unit_targets = dict(zip(range(0,self.n_units), np.zeros((self.n_units,2))))\n",
    "        self.unit_targets_previous = dict(zip(range(0,self.n_units), np.zeros((self.n_units,2))))\n",
    "        self.unit_path = dict(zip(range(0,self.n_units), [[] for i in range(0,self.n_units)]))\n",
    "        self.available_unit_ids = []\n",
    "        self.unit_moved = np.zeros((self.n_units))\n",
    "        self.prev_points = 0\n",
    "        self.prev_point_diff = 0\n",
    "        self.prev_points_increase = 0\n",
    "        self.prev_actions = np.zeros((self.env_cfg[\"max_units\"]), dtype=int)\n",
    "        self.prev_energys = 100*np.ones((self.n_units))\n",
    "        self.previous_positions = -np.ones((self.n_units,2))\n",
    "\n",
    "    def compare_positions(self, pos1, pos2):\n",
    "        return pos1[0]==pos2[0] and pos1[1]==pos2[1]\n",
    "        \n",
    "    # bunnyhop mechanic (maximize points by avoiding doubling on fragment)\n",
    "    def bunnyhop(self, unit, unit_positions):\n",
    "        counter = 0\n",
    "        unit_pos = unit_positions[unit]\n",
    "        for unit2 in range(self.n_units):            \n",
    "            if self.unit_has_target[unit2]==2 and self.tile_map.map[unit_positions[unit2][0],unit_positions[unit2][1]]!=2 and len(self.unit_path[unit])>1 and self.compare_positions(self.unit_path[unit][0],unit_positions[unit2]):\n",
    "                self.unit_path[unit2] = self.unit_path[unit][1:]\n",
    "                self.unit_targets[unit2] = self.unit_targets[unit]\n",
    "                self.unit_has_target[unit2] = 1#self.unit_has_target[unit]\n",
    "                self.unit_path[unit] = [unit_positions[unit2]]\n",
    "                self.unit_targets[unit] = unit_positions[unit2]\n",
    "                self.unit_has_target[unit] = 1\n",
    "                counter +=1\n",
    "                if counter<10:\n",
    "                    self.bunnyhop(unit2, unit_positions)\n",
    "\n",
    "    def positions_to_map(self, unit_positions):\n",
    "        unit_map = np.zeros((24,24))\n",
    "        for unit in unit_positions:\n",
    "            if unit[0]!=-1 and unit[1]!=-1:\n",
    "                unit_map[unit[0],unit[1]] = 1\n",
    "        return unit_map\n",
    "\n",
    "    # adjust for not only direct hits, but adjacent hits\n",
    "    def check_hit(self, target):\n",
    "        for pos in self.enemy_positions:\n",
    "            if pos[0]!=-1 and pos[1]!=-1:\n",
    "                if pos[0]==target[0] and pos[1]==target[1]:\n",
    "                    return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def get_init_proxy_obs(self, obs):\n",
    "         return (np.array([np.zeros((24,24),dtype=int) for i in range(6)]),np.array([0,0,0]), np.zeros((self.n_units,7),dtype=int))\n",
    "     \n",
    "    def step(self, obs, step):\n",
    "        reward = 0\n",
    "        unit_mask = np.array(obs[\"units_mask\"][self.team_id]) # shape (max_units, )\n",
    "        #print(step, unit_mask)\n",
    "        self.unit_positions = np.array(obs[\"units\"][\"position\"][self.team_id]) # shape (max_units, 2)\n",
    "        self.enemy_positions = np.array(obs[\"units\"][\"position\"][abs(self.team_id-1)]).tolist()\n",
    "        self.unit_energys = np.array(obs[\"units\"][\"energy\"][self.team_id]) # shape (max_units, 1)\n",
    "        observed_relic_node_positions = np.array(obs[\"relic_nodes\"]) # shape (max_relic_nodes, 2)\n",
    "        observed_relic_nodes_mask = np.array(obs[\"relic_nodes_mask\"]) # shape (max_relic_nodes, )\n",
    "        team_points = np.array(obs[\"team_points\"]) # points of each team, team_points[self.team_id] is the points of the your team\n",
    "        increase = team_points[self.team_id]-self.prev_points\n",
    "        diff = team_points[self.team_id] - team_points[abs(self.team_id-1)]\n",
    "        diff_change = diff-self.prev_point_diff\n",
    "        self.prev_point_diff = diff\n",
    "        # ids of units you can control at this timestep\n",
    "        current_tile_map = obs[\"map_features\"][\"tile_type\"]\n",
    "        current_energy_map = obs[\"map_features\"][\"energy\"]\n",
    "        ### proxy reward calculation ###\n",
    "        # change in point difference \n",
    "        '''reward += 10*increase\n",
    "        # units on known fragment tiles\n",
    "        if self.obs[\"team_points\"][self.team_id]>self.wins:\n",
    "            self.wins = self.obs[\"team_points\"][self.team_id]\n",
    "            retard += 1000\n",
    "        if self.obs[\"team_points\"][abs(self.team_id-1)]>self.wins:\n",
    "            self.wins = self.obs[\"team_points\"][abs(self.team_id-1)]\n",
    "            retard += -1000'''\n",
    "            \n",
    "        for unit in range(self.n_units):\n",
    "            pos = self.unit_positions[unit]\n",
    "            reward -= abs(pos[0]-4)\n",
    "            if pos[0]==4:\n",
    "                reward +=10\n",
    "            t = self.unit_targets[unit]\n",
    "            if t[0]==4:\n",
    "                reward +=10\n",
    "            if self.prev_actions[unit,0]==5:\n",
    "                reward -=10\n",
    "            '''if pos[0]!=-1 and pos[1]!=-1:\n",
    "                if self.relic_map.map_knowns[pos[0],pos[1]]==1:\n",
    "                    reward += 10\n",
    "                # units targeting possibles/known fragments\n",
    "                t = self.unit_targets[unit]\n",
    "                #print(t[0], t[1])\n",
    "                if self.relic_map.map_knowns[int(t[0]),int(t[1])]==1 or self.relic_map.map_possibles[int(t[0]),int(t[1])]==1:\n",
    "                    reward += 10\n",
    "                if self.tile_map.map[int(t[0]),int(t[1])]==-1:\n",
    "                    reward += 1'''\n",
    "            # unit dies (negative reward)\n",
    "            #else: \n",
    "            #    if self.unit_moved[unit]:\n",
    "            #        reward += -1\n",
    "            # hit enemy\n",
    "            #action = self.prev_actions[unit]\n",
    "            #print(action)\n",
    "            #if action[0]==5:\n",
    "            #    reward += self.check_hit(action[1:])\n",
    "            # collision\n",
    "            #if action[0]>0 and pos[0]==self.previous_positions[unit][0] and pos[0]==self.previous_positions[unit][1]:\n",
    "            #    reward += -10\n",
    "            #f self.compare_positions\n",
    "                \n",
    "            \n",
    "            \n",
    "        \n",
    "        if step in [102,203,304,405]:\n",
    "            self.reset()\n",
    "            \n",
    "        # visible relic nodes\n",
    "        visible_relic_node_ids = set(np.where(observed_relic_nodes_mask)[0])\n",
    "        # save any new relic nodes that we discover for the rest of the game.\n",
    "        for ii in visible_relic_node_ids:\n",
    "            if ii not in self.discovered_relic_nodes_ids:\n",
    "                # explore units switch to relic collection\n",
    "                self.relic_map.new_relic(observed_relic_node_positions[ii])\n",
    "                self.discovered_relic_nodes_ids.add(ii)\n",
    "                self.discovered_relic_nodes_ids.add((ii+3)%6)\n",
    "                self.relic_node_positions.append(observed_relic_node_positions[ii])\n",
    "        # update maps\n",
    "        self.available_unit_ids = np.where(unit_mask)[0].tolist()\n",
    "        self.relic_map.step(self.unit_positions, increase)\n",
    "        tile_shift = self.tile_map.update(current_tile_map)\n",
    "        energy_shift = self.energy_map.update(current_energy_map)        \n",
    "\n",
    "        # find out move cost\n",
    "        if step>2 and not self.move_check and self.tile_map.map[self.unit_positions[0][0],self.unit_positions[0][1]]!=1 and self.unit_moved[0]:\n",
    "            self.move_cost=self.previous_energys[0]-self.unit_energys[0]+self.energy_map.map[self.unit_positions[0][0],self.unit_positions[0][1]]\n",
    "            self.move_check=1\n",
    "        # find out nebula drain\n",
    "        if not self.nebula_check and self.move_check:\n",
    "            for unit in self.available_unit_ids:\n",
    "                if self.unit_moved[unit] and  self.tile_map.map[self.unit_positions[unit][0],self.unit_positions[unit][1]]==1:\n",
    "                    self.nebula_check=1\n",
    "                    self.nebula_drain = -(self.unit_energys[unit]-self.previous_energys[unit]-self.energy_map.map[self.unit_positions[unit][0],self.unit_positions[unit][1]]+self.move_cost)\n",
    "                    break\n",
    "\n",
    "        \n",
    "        self.previous_energys = self.unit_energys\n",
    "        self.prev_points = team_points[self.team_id]\n",
    "        self.prev_points_increase = increase\n",
    "        self.previous_positions = self.unit_positions\n",
    "\n",
    "        # TODO explore map\n",
    "        tiles = np.ones((24,24))\n",
    "        tiles[self.tile_map.map==2] = 0\n",
    "        energy = self.energy_map.map.copy()\n",
    "        energy[self.tile_map.map==1] = energy[self.tile_map.map==1] - self.nebula_drain\n",
    "        my_unit_map = self.positions_to_map(self.unit_positions)\n",
    "        enemy_unit_map = self.positions_to_map(self.enemy_positions)\n",
    "        on_known = np.zeros((self.n_units,1))\n",
    "        tile_energys = np.zeros((self.n_units,1))\n",
    "        for ii, p in enumerate(self.unit_positions):\n",
    "            if self.relic_map.map_knowns[p[0],p[1]]==1:\n",
    "                on_known[ii] = 1\n",
    "            tile_energys[ii] = energy[p[0],p[1]]\n",
    "                \n",
    "\n",
    "        proxy_obs = (np.array([tiles.astype(int), energy.astype(int), self.relic_map.map_possibles.astype(int), self.relic_map.map_knowns.astype(int), my_unit_map.astype(int), enemy_unit_map.astype(int)]), \n",
    "                     np.array([step, diff, np.sum(self.unit_energys)]), \n",
    "                     np.concatenate((np.array(self.unit_positions).astype(int), np.array(list(self.unit_targets.values())).astype(int), np.expand_dims(self.unit_energys,-1).astype(int), tile_energys.astype(int), on_known.astype(int)), axis=-1))\n",
    "        return proxy_obs, reward\n",
    "        \n",
    "    def act(self, obs, step):\n",
    "        proxy_obs, _ = self.step(obs, step)\n",
    "        proxy_action,_ = self.model.get_value_and_action(proxy_obs)\n",
    "        return proxy_to_act(proxy_action)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def proxy_to_act(self, proxy_action):\n",
    "        if torch.is_tensor(proxy_action):\n",
    "            proxy_action = proxy_action.squeeze().cpu().detach().numpy()\n",
    "        actions = np.zeros((self.n_units, 3), dtype=int)\n",
    "        for unit in self.available_unit_ids:\n",
    "            if proxy_action[unit,0]==1:\n",
    "                actions[unit] = [5, proxy_action[unit,3], proxy_action[unit,4]]\n",
    "            else:\n",
    "                self.unit_targets[unit] = [proxy_action[unit,1],proxy_action[unit,2]]\n",
    "                '''if not self.compare_positions(self.unit_targets[unit], self.unit_targets_previous[unit]):\n",
    "                    path, _ = a_star(unit_positions[unit], self.unit_targets[unit], self.tile_map.map, self.energy_map.map, self.relic_map.map_knowns, self.move_cost, self.nebula_drain, use_energy=False)\n",
    "                    self.unit_path[unit] = path[1:]'''\n",
    "                direction = direction_to(self.unit_positions[unit], self.unit_targets[unit])\n",
    "                change = direction_to_change(direction)\n",
    "                self.unit_path[unit] = [self.unit_positions[unit][0]+change[0],self.unit_positions[unit][1]+change[1]]\n",
    "                actions[unit] = [direction, 0, 0]\n",
    "\n",
    "        self.prev_actions = actions\n",
    "        self.unit_targets_previous = self.unit_targets\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac52d86a-6597-4bc2-be01-5a52261b2f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenna\\anaconda3\\envs\\lux\\Lib\\site-packages\\gymnasium\\spaces\\multi_discrete.py:214: UserWarning: \u001b[33mWARN: Getting the length of a multi-dimensional MultiDiscrete space.\u001b[0m\n",
      "  gym.logger.warn(\n",
      "c:\\Users\\lenna\\anaconda3\\envs\\lux\\Lib\\site-packages\\gymnasium\\spaces\\multi_discrete.py:214: UserWarning: \u001b[33mWARN: Getting the length of a multi-dimensional MultiDiscrete space.\u001b[0m\n",
      "  gym.logger.warn(\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "name = \"best_more_ep\"\n",
    "train(name, Args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e81dae3-7141-407b-9b17-f54656248969",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590e58e8-eb9e-41d1-84aa-7df3fb13a19c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
