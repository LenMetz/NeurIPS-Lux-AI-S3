{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e05923d-4e29-40f2-8cd5-c760dbe03685",
   "metadata": {},
   "source": [
    "## PPO design notes\n",
    "\n",
    "### Actor/critic networks design\n",
    "### Env\n",
    "-convert obs to proper obs\n",
    "\n",
    "-reward calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10b8aec7-4238-4cac-97da-335e89aec5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from IPython.display import display, Javascript\n",
    "from luxai_s3.wrappers import LuxAIS3GymEnv, RecordEpisode\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "from my_agent.lux.utils import direction_to, direction_to_change\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from maps import EnergyMap, RelicMap, TileMap\n",
    "from astar import *\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import MultiDiscrete, Discrete, Tuple\n",
    "from agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d13a2b19-3832-456e-b35d-bfdf458a88e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7593, 0.4824, 0.9285],\n",
      "        [0.1835, 0.5721, 0.5691],\n",
      "        [0.5621, 0.8385, 0.9404],\n",
      "        [0.4688, 0.9578, 0.5742],\n",
      "        [0.5567, 0.1621, 0.4108],\n",
      "        [0.1294, 0.0489, 0.4268],\n",
      "        [0.6163, 0.6996, 0.3939],\n",
      "        [0.7972, 0.6215, 0.9689],\n",
      "        [0.9725, 0.7152, 0.0961],\n",
      "        [0.0560, 0.6166, 0.4640],\n",
      "        [0.3461, 0.8050, 0.1723],\n",
      "        [0.5848, 0.8527, 0.0359],\n",
      "        [0.3035, 0.7428, 0.8109],\n",
      "        [0.4202, 0.3935, 0.4194],\n",
      "        [0.8196, 0.2978, 0.9260],\n",
      "        [0.5282, 0.8194, 0.3422]])\n",
      "tensor([2, 0, 2, 2, 1, 0, 1, 2, 0, 1, 1, 1, 1, 1, 0, 2])\n",
      "tensor([-0.9101, -1.3726, -0.9509, -1.2142, -1.3260, -1.1845, -0.9771, -0.9356,\n",
      "        -0.7837, -0.8876, -0.7715, -0.7916, -0.9987, -1.1162, -0.9954, -1.3392])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand((16,3))\n",
    "b = Categorical(logits = a)\n",
    "c = b.sample()\n",
    "d = b.log_prob(c)\n",
    "print(a)\n",
    "print(c)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddb399b4-fc30-422d-8252-0bca88f3248a",
   "metadata": {},
   "outputs": [],
   "source": [
    " class ProxyAgent():\n",
    "    def __init__(self, player: str, env_cfg) -> None:\n",
    "        self.player = player\n",
    "        self.opp_player = \"player_1\" if self.player == \"player_0\" else \"player_0\"\n",
    "        self.team_id = 0 if self.player == \"player_0\" else 1\n",
    "        self.opp_team_id = 1 if self.team_id == 0 else 0\n",
    "        np.random.seed(0)\n",
    "        self.env_cfg = env_cfg\n",
    "        if self.player==\"player_0\":\n",
    "            self.start_pos = [0,0]\n",
    "            self.pnum = 1\n",
    "        else:\n",
    "            self.start_pos = [23,23]\n",
    "            self.pnum = 0\n",
    "        self.unit_explore_locations = dict()\n",
    "        self.relic_node_positions = []\n",
    "        self.discovered_relic_nodes_ids = set()\n",
    "        self.n_units = self.env_cfg[\"max_units\"]\n",
    "        self.match_num = 1\n",
    "        self.relic_map = RelicMap(self.n_units)\n",
    "        self.tile_map = TileMap()\n",
    "        self.energy_map = EnergyMap()\n",
    "        self.move_cost = 3.0\n",
    "        self.nebula_drain = 5.0\n",
    "        self.move_check = 0\n",
    "        self.nebula_check = 0\n",
    "        \n",
    "        self.range = self.env_cfg[\"unit_sensor_range\"]\n",
    "        self.sap_range = self.env_cfg[\"unit_sap_range\"]\n",
    "        self.sap_cost = self.env_cfg[\"unit_sap_cost\"]\n",
    "        self.width = self.env_cfg[\"map_width\"]\n",
    "        self.height = self.env_cfg[\"map_height\"]\n",
    "        \n",
    "        self.unit_has_target = -np.ones((self.n_units))\n",
    "        self.unit_targets = dict(zip(range(0,self.n_units), np.zeros((self.n_units,2))))\n",
    "        self.unit_targets_previous = dict(zip(range(0,self.n_units), np.zeros((self.n_units,2))))\n",
    "        self.unit_path = dict(zip(range(0,self.n_units), [[] for i in range(0,self.n_units)]))\n",
    "        self.unit_moved = np.zeros((self.n_units))\n",
    "        self.prev_points = 0\n",
    "        self.prev_point_diff = 0\n",
    "        self.prev_points_increase = 0\n",
    "        self.prev_actions = None\n",
    "        self.previous_energys = 100*np.zeros((self.n_units))\n",
    "        self.previous_positions = -np.ones((self.n_units,2))\n",
    "\n",
    "\n",
    "    def get_explore(self, current):\n",
    "        a = np.stack((np.repeat(np.arange(24),24,axis=0).reshape((24,24)), np.repeat(np.arange(24),24,axis=0).reshape((24,24)).T),axis=2)\n",
    "        a[current!=-1] = [100,100]\n",
    "        self.explore_choices = a[np.sum(np.abs(a-np.array(self.start_pos)),axis=-1)<24-self.range].tolist()\n",
    "        if self.explore_choices:\n",
    "            return random.choice(self.explore_choices)\n",
    "        else:\n",
    "            x = np.random.randint(0,24)\n",
    "            y = np.random.randint(0,24-x)\n",
    "            return [abs(x-self.start_pos[0]), abs(y-self.start_pos[1])]\n",
    "\n",
    "    def get_moves(self, obs, unit_id, unit_pos):\n",
    "        prev_pos = [unit_pos[0] - direction_to_change(self.prev_actions[unit_id][0])[0], unit_pos[1] - direction_to_change(self.prev_actions[unit_id][0])[1]]\n",
    "        new_pos = [[unit_pos[0], unit_pos[1]-1],\n",
    "                  [unit_pos[0]+1, unit_pos[1]],\n",
    "                  [unit_pos[0], unit_pos[1]+1],\n",
    "                  [unit_pos[0]-1, unit_pos[1]]]\n",
    "        moves = [0]\n",
    "        for ii, pos in enumerate(new_pos):\n",
    "            if pos[0]<0 or pos[1]<0 or pos[0]>=self.width or pos[1]>=self.height or (pos[0]==prev_pos[0] and pos[1]==prev_pos[1]) or obs[\"map_features\"][\"tile_type\"][pos[0], pos[1]]==2 :\n",
    "            #if pos[0]<0 or pos[1]<0 or pos[0]>23 or pos[1]>23 or obs[\"map_features\"][\"tile_type\"][pos[0], pos[1]]==2:\n",
    "                pass\n",
    "            else:\n",
    "                moves.append(direction_to(unit_pos, pos))\n",
    "        #print(moves)\n",
    "        return moves\n",
    "        \n",
    "    def reset(self):\n",
    "        self.match_num += 1\n",
    "        self.unit_has_target = -np.ones((self.n_units))\n",
    "        self.unit_targets = dict(zip(range(0,self.n_units), np.zeros((self.n_units,2))))\n",
    "        self.unit_targets_previous = dict(zip(range(0,self.n_units), np.zeros((self.n_units,2))))\n",
    "        self.unit_path = dict(zip(range(0,self.n_units), [[] for i in range(0,self.n_units)]))\n",
    "        self.unit_moved = np.zeros((self.n_units))\n",
    "        self.prev_points = 0\n",
    "        self.prev_point_diff = 0\n",
    "        self.prev_points_increase = 0\n",
    "        self.prev_actions = np.zeros((self.env_cfg[\"max_units\"], 3), dtype=int)\n",
    "        self.prev_energys = 100*np.ones((self.n_units))\n",
    "        self.previous_positions = -np.ones((self.n_units,2))\n",
    "\n",
    "    def compare_positions(self, pos1, pos2):\n",
    "        return pos1[0]==pos2[0] and pos1[1]==pos2[1]\n",
    "        \n",
    "    # bunnyhop mechanic (maximize points by avoiding doubling on fragment)\n",
    "    def bunnyhop(self, unit, unit_positions):\n",
    "        counter = 0\n",
    "        unit_pos = unit_positions[unit]\n",
    "        for unit2 in range(self.n_units):            \n",
    "            if self.unit_has_target[unit2]==2 and self.tile_map.map[unit_positions[unit2][0],unit_positions[unit2][1]]!=2 and len(self.unit_path[unit])>1 and self.compare_positions(self.unit_path[unit][0],unit_positions[unit2]):\n",
    "                self.unit_path[unit2] = self.unit_path[unit][1:]\n",
    "                self.unit_targets[unit2] = self.unit_targets[unit]\n",
    "                self.unit_has_target[unit2] = 1#self.unit_has_target[unit]\n",
    "                self.unit_path[unit] = [unit_positions[unit2]]\n",
    "                self.unit_targets[unit] = unit_positions[unit2]\n",
    "                self.unit_has_target[unit] = 1\n",
    "                counter +=1\n",
    "                if counter<10:\n",
    "                    self.bunnyhop(unit2, unit_positions)\n",
    "\n",
    "    def positions_to_map(self, unit_positions):\n",
    "        unit_map = np.zeros((24,24))\n",
    "        for unit in unit_positions:\n",
    "            if unit[0]!=-1 and unit[1]!=-1:\n",
    "                unit_map[unit[0],unit[1]] = 1\n",
    "        return unit_map\n",
    "\n",
    "    # adjust for not only direct hits, but adjacent hits\n",
    "    def check_hit(self, target):\n",
    "        for pos in self.enemy_positions:\n",
    "            if pos[0]!=-1 and pos[1]!=-1:\n",
    "                if pos[0]==target[0] and pos[1]==target[1]:\n",
    "                    return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def get_init_proxy_obs(self, obs):\n",
    "         return (np.array([np.zeros((24,24),dtype=int) for i in range(6)]),np.array([0,0,0]))\n",
    "     \n",
    "    def step(self, step, obs):\n",
    "        reward = 0\n",
    "        unit_mask = np.array(obs[\"units_mask\"][self.team_id]) # shape (max_units, )\n",
    "        self.unit_positions = np.array(obs[\"units\"][\"position\"][self.team_id]) # shape (max_units, 2)\n",
    "        self.enemy_positions = np.array(obs[\"units\"][\"position\"][abs(self.team_id-1)]).tolist()\n",
    "        self.unit_energys = np.array(obs[\"units\"][\"energy\"][self.team_id]) # shape (max_units, 1)\n",
    "        observed_relic_node_positions = np.array(obs[\"relic_nodes\"]) # shape (max_relic_nodes, 2)\n",
    "        observed_relic_nodes_mask = np.array(obs[\"relic_nodes_mask\"]) # shape (max_relic_nodes, )\n",
    "        team_points = np.array(obs[\"team_points\"]) # points of each team, team_points[self.team_id] is the points of the your team\n",
    "        increase = team_points[self.team_id]-self.prev_points\n",
    "        diff = team_points[self.team_id] - team_points[abs(self.team_id-1)]\n",
    "        diff_change = diff-self.prev_point_diff\n",
    "        self.prev_point_diff = diff\n",
    "        # ids of units you can control at this timestep\n",
    "        current_tile_map = obs[\"map_features\"][\"tile_type\"]\n",
    "        current_energy_map = obs[\"map_features\"][\"energy\"]\n",
    "        ### proxy reward calculation ###\n",
    "        # change in point difference \n",
    "        reward += diff\n",
    "        # units on known fragment tiles\n",
    "        for unit in range(self.n_units):\n",
    "            pos = self.unit_positions\n",
    "            if pos[0]!=-1 and pos[1]!=-1:\n",
    "                if self.relic_map.map_knowns[pos[0],pos[1]]==1:\n",
    "                    reward += 1\n",
    "                # units targeting possibles/known fragments\n",
    "                t = self.unit_targets[unit]\n",
    "                if self.relic_map.map_knowns[t[0],t[1]]==1 or self.relic_map.map_possibles[t[0],t[1]]==1:\n",
    "                    reward += 1\n",
    "            # unit dies (negative reward)\n",
    "            else: \n",
    "                if self.unit_moved[unit]:\n",
    "                    reward += -1\n",
    "            # hit enemy\n",
    "            action = self.prev_actions[unit]\n",
    "            if action[0]==5:\n",
    "                reward += self.check_hit(action[1:])\n",
    "            \n",
    "            \n",
    "        \n",
    "        if step in [102,203,304,405]:\n",
    "            self.reset()\n",
    "            \n",
    "        # visible relic nodes\n",
    "        visible_relic_node_ids = set(np.where(observed_relic_nodes_mask)[0])\n",
    "        # save any new relic nodes that we discover for the rest of the game.\n",
    "        for ii in visible_relic_node_ids:\n",
    "            if ii not in self.discovered_relic_nodes_ids:\n",
    "                # explore units switch to relic collection\n",
    "                self.relic_map.new_relic(observed_relic_node_positions[ii])\n",
    "                self.discovered_relic_nodes_ids.add(ii)\n",
    "                self.discovered_relic_nodes_ids.add((ii+3)%6)\n",
    "                self.relic_node_positions.append(observed_relic_node_positions[ii])\n",
    "        # update maps\n",
    "        self.available_unit_ids = np.where(unit_mask)[0].tolist()\n",
    "        self.relic_map.step(unit_positions, increase)\n",
    "        tile_shift = self.tile_map.update(current_tile_map)\n",
    "        energy_shift = self.energy_map.update(current_energy_map)        \n",
    "\n",
    "        # find out move cost\n",
    "        if step>2 and not self.move_check and self.tile_map.map[unit_positions[0][0],unit_positions[0][1]]!=1 and self.unit_moved[0]:\n",
    "            self.move_cost=self.previous_energys[0]-unit_energys[0]+self.energy_map.map[unit_positions[0][0],unit_positions[0][1]]\n",
    "            self.move_check=1\n",
    "        # find out nebula drain\n",
    "        if not self.nebula_check and self.move_check:\n",
    "            for unit in available_unit_ids:\n",
    "                if self.unit_moved[unit] and  self.tile_map.map[unit_positions[unit][0],unit_positions[unit][1]]==1:\n",
    "                    self.nebula_check=1\n",
    "                    self.nebula_drain = -(unit_energys[unit]-self.previous_energys[unit]-self.energy_map.map[unit_positions[unit][0],unit_positions[unit][1]]+self.move_cost)\n",
    "                    break\n",
    "        tiles = np.ones(24,24)\n",
    "        tiles[self.tile_map.map==2] = 0\n",
    "        energy = self.energy_map.map.copy()\n",
    "        energy[self.tile_map.map==1] = energy[self.tile_map.map==1] - self.nebula_drain\n",
    "        my_unit_map = self.positions_to_map(self.unit_positions)\n",
    "        enemy_unit_map = self.positions_to_map(self.enemy_positions)\n",
    "        proxy_obs = (np.array(tiles, energy, self.relic_map.map_possibles, self.relic_map.map_knowns, my_unit_map, enemy_unit_map), np.array([step, diff, np.sum(self.unit_energys)]))\n",
    "        return proxy_obs, reward\n",
    "        \n",
    "    def act(self, proxy_action):\n",
    "        actions = np.zeros((self.n_units, 3), dtype=int)\n",
    "        for unit in self.available_unit_ids:\n",
    "            if proxy_action[unit][0]!=5:\n",
    "                self.unit_targets[unit] = [proxy_action[unit][1]],proxy_action[unit][2]\n",
    "                '''if not self.compare_positions(self.unit_targets[unit], self.unit_targets_previous[unit]):\n",
    "                    path, _ = a_star(unit_positions[unit], self.unit_targets[unit], self.tile_map.map, self.energy_map.map, self.relic_map.map_knowns, self.move_cost, self.nebula_drain, use_energy=False)\n",
    "                    self.unit_path[unit] = path[1:]'''\n",
    "                direction = direction_to(self.unit_positions[unit], self.unit_targets[unit])\n",
    "                change = direction_to_change(direction)\n",
    "                self.unit_path[unit] = [self.unit_positions[unit][0]+change[0],self.unit_positions[unit][1]+change[1]]\n",
    "                    \n",
    "        discover_flag = 0\n",
    "        # Decide on action. Follow path, if multiple units want to move to possible fragment only let one through, if attacking fire on enemy instead of moving\n",
    "        for unit in self.available_unit_ids:\n",
    "            unit_pos = self.unit_positions[unit]\n",
    "            self.bunnyhop(unit, self.unit_positions)\n",
    "            \n",
    "        for unit in self.available_unit_ids:\n",
    "            unit_pos = self.unit_positions[unit]\n",
    "            if proxy_action[unit][0]==5:\n",
    "                actions[unit] = [5,unit_pos[0]-proxy_action[unit][1],unit_pos[1]-proxy_action[unit][2]]\n",
    "            else:\n",
    "                if unit_energys[unit]<self.move_cost:\n",
    "                    actions[unit]=[0,0,0]\n",
    "                elif self.unit_path[unit]:\n",
    "                    if self.relic_map.map_possibles[self.unit_path[unit][0][0],self.unit_path[unit][0][1]]==1:\n",
    "                        if discover_flag:\n",
    "                            if self.relic_map.map_possibles[unit_pos[0],unit_pos[1]]==1:\n",
    "                                actions[unit] = self.relic_map.move_away(self.tile_map.map, [unit_pos[0],unit_pos[1]])\n",
    "                                self.unit_path[unit].insert(0, unit_pos)\n",
    "                            else:\n",
    "                                actions[unit]=[0,0,0]\n",
    "                        else:\n",
    "                            actions[unit] = [direction_to(unit_pos, self.unit_path[unit].pop(0)), 0, 0]\n",
    "                            discover_flag=1\n",
    "                    else:\n",
    "                        actions[unit] = [direction_to(unit_pos, self.unit_path[unit].pop(0)), 0, 0]\n",
    "                else:\n",
    "                    if self.relic_map.map_possibles[unit_pos[0],unit_pos[1]]==1:\n",
    "                        if discover_flag:\n",
    "                            actions[unit] = self.relic_map.move_away(self.tile_map.map, [unit_pos[0],unit_pos[1]])\n",
    "                            self.unit_path[unit].insert(0, unit_pos)\n",
    "                        else:\n",
    "                            actions[unit]=[0,0,0]\n",
    "                            discover_flag = 1\n",
    "                    else:\n",
    "                        actions[unit]=[0,0,0]\n",
    "        self.unit_targets_previous = self.unit_targets\n",
    "        self.previous_energys = unit_energys\n",
    "        self.relic_map.map_occupied = np.zeros((24,24))\n",
    "        self.prev_points = team_points[self.team_id]\n",
    "        self.prev_points_increase = increase\n",
    "        self.prev_actions = actions\n",
    "        self.previous_positions = unit_positions\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54f92b9e-509f-4b1b-bd1d-b9e1b035904e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "map_space = Tuple((\n",
    "            MultiDiscrete(np.full((24,24),24)),\n",
    "            MultiDiscrete(np.full((24,24),24)),\n",
    "            MultiDiscrete(np.full((24,24),24)),\n",
    "            MultiDiscrete(np.full((24,24),24)),\n",
    "            MultiDiscrete(np.full((24,24),24)),\n",
    "            MultiDiscrete(np.full((24,24),24)),\n",
    "        ))\n",
    "param_space = MultiDiscrete(np.array([505, 1000, 16*400]))\n",
    "observation_space = Tuple((map_space, param_space))\n",
    "print((observation_space[1]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dc24dd79-2936-494c-ad9c-8738fcfc7386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_fn():\n",
    "    return ProxyEnvironment()\n",
    "\n",
    "class ProxyEnvironment(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.n_maps = 6\n",
    "        self.n_state_params = 3\n",
    "        self.map_space = Tuple((\n",
    "            MultiDiscrete(np.full((24,24),24)),\n",
    "            MultiDiscrete(np.full((24,24),24)),\n",
    "            MultiDiscrete(np.full((24,24),24)),\n",
    "            MultiDiscrete(np.full((24,24),24)),\n",
    "            MultiDiscrete(np.full((24,24),24)),\n",
    "            MultiDiscrete(np.full((24,24),24)),\n",
    "        ))\n",
    "        self.param_space = MultiDiscrete(np.array([505, 1000, 16*400]))\n",
    "        self.observation_space = Tuple((self.map_space, self.param_space))\n",
    "        self.action_space = MultiDiscrete(np.array([5 for i in range(16)]))\n",
    "        self.env = RecordEpisode(LuxAIS3GymEnv(numpy_output=True), save_on_close=False, save_on_reset=False, save_dir=\"replays\")\n",
    "        self.obs, info  = self.env.reset()\n",
    "        self.agent1 = ProxyAgent(\"player_0\", info[\"params\"])\n",
    "        self.agent2 = Agent(\"player_1\", info[\"params\"])\n",
    "        self.current_step = 0\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "        \n",
    "    def reset(self, seed, options):\n",
    "        self.current_step = 0\n",
    "        self.obs, info = self.env.reset(seed=seed)\n",
    "        self.agent1 = ProxyAgent(\"player_0\", info[\"params\"])\n",
    "        self.agent2 = Agent(\"player_1\", info[\"params\"])\n",
    "        self.proxy_obs = self.agent1.get_init_proxy_obs(self.obs)\n",
    "        return self.proxy_obs, info\n",
    "\n",
    "    def step(self, proxy_action):\n",
    "        self.current_step += 1\n",
    "        actions = dict()\n",
    "        actions[\"player_0\"] = self.agent1.act(proxy_action)\n",
    "        actions[\"player_1\"] = self.agent2.act(step=self.current_step, obs=self.obs[agent.player])\n",
    "        self.obs, reward, terminated, truncated, info = env.act(actions)\n",
    "        self.proxy_obs, self.proxy_reward = env.step(self.obs, self.current_step)\n",
    "        dones = {k: terminated[k] | truncated[k] for k in terminated}\n",
    "        if dones[\"player_0\"] or dones[\"player_1\"]:\n",
    "            game_done = True\n",
    "        return self.proxy_obs, self.proxy_reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a468f572-cd3c-4ae8-bb5b-5096d92db788",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    exp_name: str =\"\"\n",
    "    \"\"\"the name of this experiment\"\"\"\n",
    "    seed: int = 1\n",
    "    \"\"\"seed of the experiment\"\"\"\n",
    "    torch_deterministic: bool = True\n",
    "    \"\"\"if toggled, `torch.backends.cudnn.deterministic=False`\"\"\"\n",
    "    cuda: bool = False\n",
    "    \"\"\"if toggled, cuda will be enabled by default\"\"\"\n",
    "    capture_video: bool = False\n",
    "    \"\"\"whether to capture videos of the agent performances (check out `videos` folder)\"\"\"\n",
    "\n",
    "    # Algorithm specific arguments\n",
    "    env_id: str = \"CartPole-v1\"\n",
    "    \"\"\"the id of the environment\"\"\"\n",
    "    total_timesteps: int = 500000\n",
    "    \"\"\"total timesteps of the experiments\"\"\"\n",
    "    learning_rate: float = 2.5e-4\n",
    "    \"\"\"the learning rate of the optimizer\"\"\"\n",
    "    num_envs: int = 4\n",
    "    \"\"\"the number of parallel game environments\"\"\"\n",
    "    num_steps: int = 128\n",
    "    \"\"\"the number of steps to run in each environment per policy rollout\"\"\"\n",
    "    anneal_lr: bool = True\n",
    "    \"\"\"Toggle learning rate annealing for policy and value networks\"\"\"\n",
    "    gamma: float = 0.99\n",
    "    \"\"\"the discount factor gamma\"\"\"\n",
    "    gae_lambda: float = 0.95\n",
    "    \"\"\"the lambda for the general advantage estimation\"\"\"\n",
    "    num_minibatches: int = 4\n",
    "    \"\"\"the number of mini-batches\"\"\"\n",
    "    update_epochs: int = 4\n",
    "    \"\"\"the K epochs to update the policy\"\"\"\n",
    "    norm_adv: bool = True\n",
    "    \"\"\"Toggles advantages normalization\"\"\"\n",
    "    clip_coef: float = 0.2\n",
    "    \"\"\"the surrogate clipping coefficient\"\"\"\n",
    "    clip_vloss: bool = True\n",
    "    \"\"\"Toggles whether or not to use a clipped loss for the value function, as per the paper.\"\"\"\n",
    "    ent_coef: float = 0.01\n",
    "    \"\"\"coefficient of the entropy\"\"\"\n",
    "    vf_coef: float = 0.5\n",
    "    \"\"\"coefficient of the value function\"\"\"\n",
    "    max_grad_norm: float = 0.5\n",
    "    \"\"\"the maximum norm for the gradient clipping\"\"\"\n",
    "    target_kl: float = None\n",
    "    \"\"\"the target KL divergence threshold\"\"\"\n",
    "\n",
    "    # to be filled in runtime\n",
    "    batch_size: int = 0\n",
    "    \"\"\"the batch size (computed in runtime)\"\"\"\n",
    "    minibatch_size: int = 0\n",
    "    \"\"\"the mini-batch size (computed in runtime)\"\"\"\n",
    "    num_iterations: int = 0\n",
    "    \"\"\"the number of iterations (computed in runtime)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6e16e823-e2c3-4958-babd-689b9ecf6f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "# TODO network design\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.n_ens = env.observation_space[1].shape[0]\n",
    "        n_maps = len(env.single_observation_space[0])\n",
    "        n_state_params = env.single_observation_space[1].shape[0]\n",
    "        print(n_state_params)\n",
    "        self.map_to_hidden = nn.Sequential(\n",
    "            nn.Conv2d(n_maps, 12, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(12, 6, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            layer_init(nn.Linear(6*6*6, 128)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.state_params_to_hidden = nn.Sequential(\n",
    "            layer_init(nn.Linear(n_state_params, 64)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.map_and_state_params_combine = nn.Sequential(\n",
    "            layer_init(nn.Linear(128 + 64, 64)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            layer_init(nn.Linear(64, 128)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(128, 16*5)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(64, 16)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(16, 1)),\n",
    "        )\n",
    "\n",
    "    def combine(self, x):\n",
    "        maps, state_params = x\n",
    "        map_hidden = self.map_to_hidden(maps)\n",
    "        state_params_hidden = self.state_params_to_hidden(state_params)\n",
    "        return self.map_and_state_params_combine(torch.cat((map_hidden, state_params_hidden), dim=-1))\n",
    "    \n",
    "    def get_value(self, x):\n",
    "        return self.critic(self.combine(x))\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        x = self.combine(x)\n",
    "        logits = self.actor(x).reshape(self.n_ens, 16,5)\n",
    "        #print(logits)\n",
    "        probs = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0d098b1d-d3a8-4839-8ae1-23e58c93381f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(exp_name, args):\n",
    "    args.exp_name = exp_name\n",
    "    args.batch_size = int(args.num_envs * args.num_steps)\n",
    "    args.minibatch_size = int(args.batch_size // args.num_minibatches)\n",
    "    args.num_iterations = args.total_timesteps // args.batch_size\n",
    "    run_name = f\"{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "    writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "    writer.add_text(\n",
    "        \"hyperparameters\",\n",
    "        \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
    "    )\n",
    "\n",
    "    # TRY NOT TO MODIFY: seeding\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "\n",
    "    # env setup\n",
    "    envs = gym.vector.SyncVectorEnv(\n",
    "        [env_fn for i in range(args.num_envs)],\n",
    "    )\n",
    "\n",
    "    agent = ActorCritic(envs).to(device)\n",
    "    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)\n",
    "    # ALGO Logic: Storage setup\n",
    "    obs = (torch.zeros((args.num_steps, args.num_envs) + np.array(envs.single_observation_space[0]).shape).to(device),torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space[1].shape).to(device))\n",
    "    actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)\n",
    "    logprobs = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)\n",
    "    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    values = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "\n",
    "    # TRY NOT TO MODIFY: start the game\n",
    "    global_step = 0\n",
    "    start_time = time.time()\n",
    "    next_obs, _ = envs.reset(seed=args.seed)\n",
    "    next_obs = (torch.Tensor(next_obs[0]).to(device).reshape((args.num_envs,) + np.array(envs.single_observation_space[0]).shape),\n",
    "                torch.Tensor(next_obs[1]).reshape((args.num_envs,)+envs.single_observation_space[1].shape).to(device))\n",
    "    next_done = torch.zeros(args.num_envs).to(device)\n",
    "    for iteration in range(1, args.num_iterations + 1):\n",
    "        # Annealing the rate if instructed to do so.\n",
    "        if args.anneal_lr:\n",
    "            frac = 1.0 - (iteration - 1.0) / args.num_iterations\n",
    "            lrnow = frac * args.learning_rate\n",
    "            optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "        for step in range(0, args.num_steps):\n",
    "            global_step += args.num_envs\n",
    "            obs[0][step] = next_obs[0]\n",
    "            obs[1][step] = next_obs[1]\n",
    "            dones[step] = next_done\n",
    "            # ALGO LOGIC: action logic\n",
    "            with torch.no_grad():\n",
    "                action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
    "                values[step] = value.flatten()\n",
    "            actions[step] = action\n",
    "            logprobs[step] = logprob\n",
    "\n",
    "            # TRY NOT TO MODIFY: execute the game and log data.\n",
    "            next_obs, reward, terminations, truncations, infos = envs.step(action.cpu().numpy())\n",
    "            next_done = np.logical_or(terminations, truncations)\n",
    "            rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "            next_obs = (torch.Tensor(next_obs[0]).to(device).reshape((args.num_envs,) + np.array(envs.single_observation_space[0]).shape),\n",
    "                        torch.Tensor(next_obs[1]).reshape((args.num_envs,)+envs.single_observation_space[1].shape).to(device))\n",
    "            next_done = torch.Tensor(next_done).to(device)\n",
    "\n",
    "            if \"final_info\" in infos:\n",
    "                for info in infos[\"final_info\"]:\n",
    "                    if info and \"episode\" in info:\n",
    "                        print(f\"global_step={global_step}, episodic_return={info['episode']['r']}\")\n",
    "                        writer.add_scalar(\"charts/episodic_return\", info[\"episode\"][\"r\"], global_step)\n",
    "                        writer.add_scalar(\"charts/episodic_length\", info[\"episode\"][\"l\"], global_step)\n",
    "\n",
    "        # bootstrap value if not done\n",
    "        with torch.no_grad():\n",
    "            next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "            advantages = torch.zeros_like(rewards).to(device)\n",
    "            lastgaelam = 0\n",
    "            for t in reversed(range(args.num_steps)):\n",
    "                if t == args.num_steps - 1:\n",
    "                    nextnonterminal = 1.0 - next_done\n",
    "                    nextvalues = next_value\n",
    "                else:\n",
    "                    nextnonterminal = 1.0 - dones[t + 1]\n",
    "                    nextvalues = values[t + 1]\n",
    "                delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]\n",
    "                advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n",
    "            returns = advantages + values\n",
    "\n",
    "        # flatten the batch\n",
    "        b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "        b_logprobs = logprobs.reshape(-1)\n",
    "        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "        b_advantages = advantages.reshape(-1)\n",
    "        b_returns = returns.reshape(-1)\n",
    "        b_values = values.reshape(-1)\n",
    "\n",
    "        # Optimizing the policy and value network\n",
    "        b_inds = np.arange(args.batch_size)\n",
    "        clipfracs = []\n",
    "        for epoch in range(args.update_epochs):\n",
    "            np.random.shuffle(b_inds)\n",
    "            for start in range(0, args.batch_size, args.minibatch_size):\n",
    "                end = start + args.minibatch_size\n",
    "                mb_inds = b_inds[start:end]\n",
    "\n",
    "                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions.long()[mb_inds])\n",
    "                logratio = newlogprob - b_logprobs[mb_inds]\n",
    "                ratio = logratio.exp()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                    old_approx_kl = (-logratio).mean()\n",
    "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n",
    "\n",
    "                mb_advantages = b_advantages[mb_inds]\n",
    "                if args.norm_adv:\n",
    "                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "                # Policy loss\n",
    "                pg_loss1 = -mb_advantages * ratio\n",
    "                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n",
    "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                # Value loss\n",
    "                newvalue = newvalue.view(-1)\n",
    "                if args.clip_vloss:\n",
    "                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
    "                    v_clipped = b_values[mb_inds] + torch.clamp(\n",
    "                        newvalue - b_values[mb_inds],\n",
    "                        -args.clip_coef,\n",
    "                        args.clip_coef,\n",
    "                    )\n",
    "                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                    v_loss = 0.5 * v_loss_max.mean()\n",
    "                else:\n",
    "                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "                entropy_loss = entropy.mean()\n",
    "                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "\n",
    "            if args.target_kl is not None and approx_kl > args.target_kl:\n",
    "                break\n",
    "\n",
    "        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "        var_y = np.var(y_true)\n",
    "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "        # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
    "        writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n",
    "        writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n",
    "        writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n",
    "        writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n",
    "        writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n",
    "        print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "        writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "\n",
    "    envs.close()\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ac52d86a-6597-4bc2-be01-5a52261b2f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenna\\anaconda3\\envs\\lux\\Lib\\site-packages\\gymnasium\\spaces\\multi_discrete.py:214: UserWarning: \u001b[33mWARN: Getting the length of a multi-dimensional MultiDiscrete space.\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ProxyAgent' object has no attribute 'available_unit_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbasic_test\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mArgs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[45], line 63\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(exp_name, args)\u001b[0m\n\u001b[0;32m     60\u001b[0m logprobs[step] \u001b[38;5;241m=\u001b[39m logprob\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# TRY NOT TO MODIFY: execute the game and log data.\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m next_obs, reward, terminations, truncations, infos \u001b[38;5;241m=\u001b[39m \u001b[43menvs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m next_done \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlogical_or(terminations, truncations)\n\u001b[0;32m     65\u001b[0m rewards[step] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(reward)\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lenna\\anaconda3\\envs\\lux\\Lib\\site-packages\\gymnasium\\vector\\sync_vector_env.py:222\u001b[0m, in \u001b[0;36mSyncVectorEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_truncations[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     (\n\u001b[0;32m    217\u001b[0m         env_obs,\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rewards[i],\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_terminations[i],\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_truncations[i],\n\u001b[0;32m    221\u001b[0m         env_info,\n\u001b[1;32m--> 222\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    224\u001b[0m observations\u001b[38;5;241m.\u001b[39mappend(env_obs)\n\u001b[0;32m    225\u001b[0m infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_info(infos, env_info, i)\n",
      "Cell \u001b[1;32mIn[42], line 39\u001b[0m, in \u001b[0;36mProxyEnvironment.step\u001b[1;34m(self, proxy_action)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     38\u001b[0m actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m---> 39\u001b[0m actions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplayer_0\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproxy_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m actions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplayer_1\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent2\u001b[38;5;241m.\u001b[39mact(step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_step, obs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs[agent\u001b[38;5;241m.\u001b[39mplayer])\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mact(actions)\n",
      "Cell \u001b[1;32mIn[3], line 207\u001b[0m, in \u001b[0;36mProxyAgent.act\u001b[1;34m(self, proxy_action)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mact\u001b[39m(\u001b[38;5;28mself\u001b[39m, proxy_action):\n\u001b[0;32m    206\u001b[0m     actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_units, \u001b[38;5;241m3\u001b[39m), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m--> 207\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m unit \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavailable_unit_ids\u001b[49m:\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m proxy_action[unit][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m5\u001b[39m:\n\u001b[0;32m    209\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_targets[unit] \u001b[38;5;241m=\u001b[39m [proxy_action[unit][\u001b[38;5;241m1\u001b[39m]],proxy_action[unit][\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ProxyAgent' object has no attribute 'available_unit_ids'"
     ]
    }
   ],
   "source": [
    "name = \"basic_test\"\n",
    "train(name, Args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d4c814-05c7-4cc3-ab27-281d34223b33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a74a608-328d-464e-b6ec-87916bc6e43a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
